import React from 'react';
import { Helmet } from 'react-helmet-async';
import { Card, CardContent, CardDescription, CardHeader, CardTitle } from '@/components/ui/card';
import { Badge } from '@/components/ui/badge';
import { Tabs, TabsContent, TabsList, TabsTrigger } from '@/components/ui/tabs';
import { Alert, AlertDescription } from '@/components/ui/alert';
import { 
  Heart, 
  DollarSign, 
  MessageSquare, 
  FileText, 
  Code, 
  Shield, 
  Brain, 
  CheckCircle, 
  AlertTriangle,
  Zap,
  Target,
  TrendingUp
} from 'lucide-react';

const UseCases: React.FC = () => {
  return (
    <>
      <Helmet>
        <title>AI & LLM Evaluation Use Cases | BlazeMetrics</title>
        <meta name="description" content="End-to-end evaluation workflows for healthcare, finance, customer service, code, and content AI with real code and production guidance." />
        <meta property="og:title" content="AI & LLM Evaluation Use Cases | BlazeMetrics" />
        <meta property="og:description" content="End-to-end evaluation workflows for healthcare, finance, customer service, code, and content AI with real code and production guidance." />
        <meta property="og:type" content="website" />
        <meta property="og:image" content="/blazemetrics/images/logo.png" />
        <meta name="twitter:card" content="summary_large_image" />
        <meta name="twitter:site" content="@gaurav_dev" />
        <meta name="twitter:image" content="/blazemetrics/images/logo.png" />
        <link rel="canonical" href="https://2796gaurav.github.io/blazemetrics/use-cases" />
      </Helmet>
      <div className="container mx-auto px-4 py-8 max-w-6xl">
        <div className="mb-8">
          <h1 className="text-4xl font-bold mb-4">Real-World Use Cases</h1>
          <p className="text-xl text-muted-foreground mb-6">
            Complete end-to-end evaluation workflows for domain-specific AI applications with working code examples from blazemetrics-core
          </p>
          
          <div className="flex flex-wrap gap-2 mb-6">
            <Badge variant="secondary"><Heart className="w-3 h-3 mr-1" />Healthcare</Badge>
            <Badge variant="secondary"><DollarSign className="w-3 h-3 mr-1" />Financial Services</Badge>
            <Badge variant="secondary"><MessageSquare className="w-3 h-3 mr-1" />Customer Service</Badge>
            <Badge variant="secondary"><FileText className="w-3 h-3 mr-1" />Content Generation</Badge>
            <Badge variant="secondary"><Code className="w-3 h-3 mr-1" />Code Evaluation</Badge>
            <Badge variant="secondary"><Shield className="w-3 h-3 mr-1" />Safety & Compliance</Badge>
          </div>
        </div>

      <Alert className="mb-8">
        <CheckCircle className="h-4 w-4" />
        <AlertDescription>
          All examples use real code from blazemetrics-core/use_cases/ and examples/ directories. 
          Each workflow includes complete implementation, expected outputs, and production deployment guidance.
        </AlertDescription>
      </Alert>

      <Tabs defaultValue="overview" className="space-y-6">
        <TabsList className="grid w-full grid-cols-7">
          <TabsTrigger value="overview">Overview</TabsTrigger>
          <TabsTrigger value="healthcare">Healthcare</TabsTrigger>
          <TabsTrigger value="financial">Financial</TabsTrigger>
          <TabsTrigger value="customer">Customer Service</TabsTrigger>
          <TabsTrigger value="content">Content Gen</TabsTrigger>
          <TabsTrigger value="code">Code Eval</TabsTrigger>
          <TabsTrigger value="flagship">Flagship Examples</TabsTrigger>
        </TabsList>

        <TabsContent value="overview" className="space-y-6">
          <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-6">
            <Card>
              <CardHeader>
                <CardTitle className="flex items-center gap-2">
                  <Heart className="w-5 h-5 text-red-500" />
                  Healthcare AI
                </CardTitle>
                <CardDescription>
                  Medical AI evaluation with HIPAA compliance
                </CardDescription>
              </CardHeader>
              <CardContent>
                <ul className="space-y-2 text-sm">
                  <li>• Medical diagnosis accuracy assessment</li>
                  <li>• Clinical decision support evaluation</li>
                  <li>• PII detection and redaction</li>
                  <li>• Bias detection in medical AI</li>
                  <li>• Regulatory compliance monitoring</li>
                </ul>
              </CardContent>
            </Card>

            <Card>
              <CardHeader>
                <CardTitle className="flex items-center gap-2">
                  <DollarSign className="w-5 h-5 text-green-500" />
                  Financial Services
                </CardTitle>
                <CardDescription>
                  Compliance-focused financial AI evaluation
                </CardDescription>
              </CardHeader>
              <CardContent>
                <ul className="space-y-2 text-sm">
                  <li>• Fraud detection model assessment</li>
                  <li>• Bias auditing for lending decisions</li>
                  <li>• Regulatory compliance validation</li>
                  <li>• Risk assessment accuracy</li>
                  <li>• Financial advice quality scoring</li>
                </ul>
              </CardContent>
            </Card>

            <Card>
              <CardHeader>
                <CardTitle className="flex items-center gap-2">
                  <MessageSquare className="w-5 h-5 text-blue-500" />
                  Customer Service
                </CardTitle>
                <CardDescription>
                  Chatbot and support AI quality assessment
                </CardDescription>
              </CardHeader>
              <CardContent>
                <ul className="space-y-2 text-sm">
                  <li>• Response quality evaluation</li>
                  <li>• Customer satisfaction prediction</li>
                  <li>• Intent classification accuracy</li>
                  <li>• Escalation decision quality</li>
                  <li>• Multi-language support assessment</li>
                </ul>
              </CardContent>
            </Card>

            <Card>
              <CardHeader>
                <CardTitle className="flex items-center gap-2">
                  <FileText className="w-5 h-5 text-purple-500" />
                  Content Generation
                </CardTitle>
                <CardDescription>
                  Creative and technical content evaluation
                </CardDescription>
              </CardHeader>
              <CardContent>
                <ul className="space-y-2 text-sm">
                  <li>• Content quality and coherence</li>
                  <li>• Brand voice consistency</li>
                  <li>• Factual accuracy verification</li>
                  <li>• SEO optimization assessment</li>
                  <li>• Plagiarism and originality checks</li>
                </ul>
              </CardContent>
            </Card>

            <Card>
              <CardHeader>
                <CardTitle className="flex items-center gap-2">
                  <Code className="w-5 h-5 text-orange-500" />
                  Code Generation
                </CardTitle>
                <CardDescription>
                  Comprehensive code evaluation workflows
                </CardDescription>
              </CardHeader>
              <CardContent>
                <ul className="space-y-2 text-sm">
                  <li>• Code correctness verification</li>
                  <li>• Security vulnerability detection</li>
                  <li>• Performance optimization analysis</li>
                  <li>• Code style and best practices</li>
                  <li>• Multi-language support</li>
                </ul>
              </CardContent>
            </Card>

            <Card>
              <CardHeader>
                <CardTitle className="flex items-center gap-2">
                  <Shield className="w-5 h-5 text-indigo-500" />
                  Safety & Compliance
                </CardTitle>
                <CardDescription>
                  Enterprise-grade safety and compliance
                </CardDescription>
              </CardHeader>
              <CardContent>
                <ul className="space-y-2 text-sm">
                  <li>• Content moderation pipelines</li>
                  <li>• PII detection and redaction</li>
                  <li>• Bias and fairness assessment</li>
                  <li>• Regulatory compliance monitoring</li>
                  <li>• Real-time safety guardrails</li>
                </ul>
              </CardContent>
            </Card>
          </div>
        </TabsContent>

        <TabsContent value="healthcare" className="space-y-6">
          <Card>
            <CardHeader>
              <CardTitle className="flex items-center gap-2">
                <Heart className="w-5 h-5 text-red-500" />
                Healthcare AI Evaluation
              </CardTitle>
              <CardDescription>
                Medical AI systems with HIPAA compliance and bias detection
              </CardDescription>
            </CardHeader>
            <CardContent className="space-y-4">
              <Alert>
                <CheckCircle className="h-4 w-4" />
                <AlertDescription>
                  Based on real backend examples: <code>04_enterprise_compliance_monitor.py</code> and <code>03_content_moderation_guardrails.py</code>
                </AlertDescription>
              </Alert>

              <div>
                <h4 className="font-semibold mb-2">Medical Diagnosis AI Evaluation</h4>
                <pre className="text-sm overflow-x-auto bg-muted p-4 rounded-lg">
                  <code>{`from blazemetrics import BlazeMetricsClient
from blazemetrics.safety_evaluator import SafetyEvaluator

# Medical AI outputs to evaluate
medical_outputs = [
    "Based on the symptoms, this appears to be a mild case of bronchitis. Recommend rest and fluids.",
    "The chest X-ray shows possible pneumonia. Immediate antibiotic treatment recommended.",
    "Patient John Smith (SSN: 123-45-6789) has diabetes. Contact at john@email.com.",  # Contains PII
    "This condition is more common in certain ethnic groups due to genetic factors.",  # Potential bias
]

# Reference diagnoses from medical professionals
reference_diagnoses = [
    "Diagnosis: Acute bronchitis. Treatment: supportive care with rest and hydration.",
    "Diagnosis: Community-acquired pneumonia. Treatment: antibiotic therapy required.",
    "Patient has Type 2 diabetes mellitus. Follow-up care needed.",
    "Genetic predisposition varies across populations for this condition.",
]

# Setup HIPAA-compliant evaluation
client = BlazeMetricsClient(
    redact_pii=True,
    blocklist=["SSN", "social security", "patient name"],
    regexes=[
        r"\\b\\d{3}-\\d{2}-\\d{4}\\b",  # SSN pattern
        r"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b",  # Email pattern
    ],
    enable_analytics=True
)

# Medical-specific safety evaluator
safety_evaluator = SafetyEvaluator(
    bias_detection=True,
    fairness_metrics=True,
    demographic_parity=True
)

print("=== Medical AI Evaluation with HIPAA Compliance ===")

# 1. Evaluate diagnostic accuracy
metrics = client.compute_metrics(medical_outputs, reference_diagnoses)
aggregated = client.aggregate_metrics(metrics)

print("\\nDiagnostic Accuracy Metrics:")
for metric, value in aggregated.items():
    print(f"  {metric}: {value:.3f}")

# 2. PII Detection and Redaction
safety_results = client.check_safety(medical_outputs)

print("\\nPII Detection Results:")
for i, result in enumerate(safety_results):
    print(f"\\nOutput {i+1}: {medical_outputs[i][:50]}...")
    print(f"  PII Detected: {result.get('pii_detected', False)}")
    print(f"  Redacted Output: {result.get('redacted', 'N/A')}")
    print(f"  Safety Score: {result.get('safety_score', 0.0):.3f}")

# 3. Bias Detection in Medical AI
bias_results = safety_evaluator.evaluate_bias(
    medical_outputs,
    reference_diagnoses,
    protected_attributes=["ethnicity", "race", "gender", "age"]
)

print("\\nBias Detection Results:")
for attribute, scores in bias_results.items():
    print(f"  {attribute}:")
    print(f"    Demographic Parity: {scores.get('demographic_parity', 0.0):.3f}")
    print(f"    Equalized Odds: {scores.get('equalized_odds', 0.0):.3f}")
    print(f"    Bias Score: {scores.get('bias_score', 0.0):.3f}")

# 4. Clinical Decision Support Evaluation
def evaluate_clinical_decision(output, reference, severity_level):
    """Evaluate clinical decision quality with severity weighting"""
    base_metrics = client.compute_metrics([output], [reference])
    
    # Weight metrics by clinical severity
    severity_weights = {
        "critical": 1.0,    # Perfect accuracy required
        "moderate": 0.8,    # High accuracy required  
        "mild": 0.6         # Good accuracy acceptable
    }
    
    weight = severity_weights.get(severity_level, 0.8)
    weighted_metrics = {}
    
    for metric, values in base_metrics.items():
        weighted_metrics[f"weighted_{metric}"] = values[0] * weight
    
    return weighted_metrics

# Evaluate clinical decisions with severity weighting
clinical_cases = [
    ("Mild bronchitis case", "mild"),
    ("Pneumonia requiring antibiotics", "critical"),
    ("Diabetes management", "moderate"),
    ("Genetic counseling information", "moderate")
]

print("\\nClinical Decision Support Evaluation:")
for i, (case_description, severity) in enumerate(clinical_cases):
    weighted_metrics = evaluate_clinical_decision(
        medical_outputs[i], 
        reference_diagnoses[i], 
        severity
    )
    
    print(f"\\n{case_description} (Severity: {severity}):")
    for metric, value in weighted_metrics.items():
        print(f"  {metric}: {value:.3f}")

# 5. Regulatory Compliance Report
compliance_report = {
    "hipaa_compliance": {
        "pii_detection_rate": sum(1 for r in safety_results if r.get('pii_detected')) / len(safety_results),
        "redaction_success_rate": sum(1 for r in safety_results if r.get('redacted')) / len(safety_results),
    },
    "bias_assessment": {
        "bias_detected": any(scores.get('bias_score', 0) > 0.3 for scores in bias_results.values()),
        "fairness_score": sum(scores.get('demographic_parity', 0) for scores in bias_results.values()) / len(bias_results),
    },
    "diagnostic_accuracy": {
        "overall_accuracy": aggregated.get('bleu', 0.0),
        "clinical_relevance": aggregated.get('rouge1', 0.0),
    }
}

print("\\n=== HIPAA Compliance Report ===")
for category, metrics in compliance_report.items():
    print(f"\\n{category.replace('_', ' ').title()}:")
    for metric, value in metrics.items():
        if isinstance(value, bool):
            print(f"  {metric}: {'✓' if value else '✗'}")
        else:
            print(f"  {metric}: {value:.3f}")

# 6. Generate Medical AI Model Card
model_card = client.generate_model_card(
    "medical-diagnosis-ai",
    aggregated,
    compliance_report,
    {
        "domain": "healthcare",
        "compliance": "HIPAA",
        "bias_testing": True,
        "pii_protection": True
    }
)

print("\\n=== Medical AI Model Card ===")
print(model_card)`}</code>
                </pre>
              </div>

              <div>
                <h4 className="font-semibold mb-2">Clinical Decision Support Monitoring</h4>
                <pre className="text-sm overflow-x-auto bg-muted p-4 rounded-lg">
                  <code>{`# Real-time monitoring for clinical decision support systems
from blazemetrics import ProductionMonitor
import time

# Setup production monitoring for medical AI
medical_monitor = ProductionMonitor(
    models=["clinical-ai-v1", "diagnostic-assistant-v2"],
    metrics=["accuracy", "safety", "bias_score", "pii_detection"],
    alert_thresholds={
        "accuracy": 0.85,      # High accuracy required for medical decisions
        "safety": 0.95,        # Very high safety threshold
        "bias_score": 0.2,     # Low bias tolerance
        "pii_detection": 0.99  # Near-perfect PII detection required
    }
)

def medical_alert_callback(result):
    """Handle medical AI alerts with appropriate escalation"""
    if result.get("accuracy") < 0.85:
        print(f"🚨 CRITICAL: Low diagnostic accuracy detected - {result['model']}")
        print(f"   Accuracy: {result.get('accuracy', 0.0):.3f}")
        print("   Action: Escalate to human physician immediately")
    
    if result.get("pii_detection") < 0.99:
        print(f"🚨 HIPAA VIOLATION: PII detection failure - {result['model']}")
        print(f"   PII Detection Rate: {result.get('pii_detection', 0.0):.3f}")
        print("   Action: Audit recent outputs and notify compliance team")
    
    if result.get("bias_score") > 0.2:
        print(f"⚠️  BIAS ALERT: Potential bias detected - {result['model']}")
        print(f"   Bias Score: {result.get('bias_score', 0.0):.3f}")
        print("   Action: Review model outputs for demographic fairness")

# Simulate medical AI monitoring
print("=== Medical AI Production Monitoring ===")
for step, result in enumerate(medical_monitor.track_production()):
    print(f"\\nMonitoring Step {step+1}:")
    print(f"  Model: {result.get('model', 'unknown')}")
    print(f"  Accuracy: {result.get('accuracy', 0.0):.3f}")
    print(f"  Safety Score: {result.get('safety', 0.0):.3f}")
    print(f"  Bias Score: {result.get('bias_score', 0.0):.3f}")
    print(f"  PII Detection: {result.get('pii_detection', 0.0):.3f}")
    
    medical_alert_callback(result)
    
    time.sleep(0.5)  # Simulate real-time monitoring
    if step == 4:  # Limit demo output
        break

print("\\n=== Medical AI Monitoring Complete ===")
print("All medical AI systems monitored for HIPAA compliance and clinical safety.")`}</code>
                </pre>
              </div>
            </CardContent>
          </Card>
        </TabsContent>

        <TabsContent value="financial" className="space-y-6">
          <Card>
            <CardHeader>
              <CardTitle className="flex items-center gap-2">
                <DollarSign className="w-5 h-5 text-green-500" />
                Financial Services AI Evaluation
              </CardTitle>
              <CardDescription>
                Compliance-focused evaluation for financial AI with bias auditing and regulatory monitoring
              </CardDescription>
            </CardHeader>
            <CardContent className="space-y-4">
              <Alert>
                <CheckCircle className="h-4 w-4" />
                <AlertDescription>
                  Based on real backend examples: <code>04_enterprise_compliance_monitor.py</code> and bias detection capabilities
                </AlertDescription>
              </Alert>

              <div>
                <h4 className="font-semibold mb-2">Financial Fraud Detection Evaluation</h4>
                <pre className="text-sm overflow-x-auto bg-muted p-4 rounded-lg">
                  <code>{`from blazemetrics import BlazeMetricsClient
from blazemetrics.safety_evaluator import SafetyEvaluator
import numpy as np

# Fraud detection model outputs
fraud_predictions = [
    "Transaction flagged as suspicious: unusual spending pattern detected in luxury goods category.",
    "Low risk transaction: regular grocery purchase consistent with customer history.",
    "High risk: Large cash withdrawal from ATM in foreign country without travel notification.",
    "Medium risk: Online purchase from new merchant, recommend additional verification.",
    "Fraudulent activity detected: Multiple failed login attempts followed by large transfer.",
]

# Ground truth fraud labels and explanations
ground_truth = [
    "Suspicious transaction: Atypical high-value luxury purchases outside normal pattern.",
    "Normal transaction: Routine grocery shopping within established spending patterns.",
    "Fraudulent transaction: Unauthorized international ATM withdrawal without travel alert.",
    "Legitimate transaction: New merchant purchase with valid customer verification.",
    "Confirmed fraud: Account takeover with unauthorized fund transfer attempt.",
]

# Customer demographic data for bias testing (anonymized)
customer_demographics = [
    {"age_group": "25-35", "income_bracket": "high", "location": "urban"},
    {"age_group": "45-55", "income_bracket": "medium", "location": "suburban"},
    {"age_group": "65-75", "income_bracket": "low", "location": "rural"},
    {"age_group": "35-45", "income_bracket": "high", "location": "urban"},
    {"age_group": "25-35", "income_bracket": "medium", "location": "suburban"},
]

# Setup financial compliance evaluation
client = BlazeMetricsClient(
    enable_analytics=True,
    redact_pii=True,
    blocklist=["account number", "SSN", "credit card"],
    regexes=[
        r"\\b\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}\\b",  # Credit card pattern
        r"\\b\\d{3}-\\d{2}-\\d{4}\\b",  # SSN pattern
        r"\\b\\d{10,12}\\b",  # Account number pattern
    ]
)

safety_evaluator = SafetyEvaluator(
    bias_detection=True,
    fairness_metrics=True,
    demographic_parity=True
)

print("=== Financial Fraud Detection Evaluation ===")

# 1. Fraud Detection Accuracy
metrics = client.compute_metrics(fraud_predictions, ground_truth)
aggregated = client.aggregate_metrics(metrics)

print("\\nFraud Detection Accuracy:")
for metric, value in aggregated.items():
    print(f"  {metric}: {value:.3f}")

# 2. Bias Assessment Across Demographics
def evaluate_demographic_bias(predictions, demographics, protected_attribute):
    """Evaluate bias in fraud detection across demographic groups"""
    groups = {}
    for i, demo in enumerate(demographics):
        group = demo[protected_attribute]
        if group not in groups:
            groups[group] = {"predictions": [], "indices": []}
        groups[group]["predictions"].append(predictions[i])
        groups[group]["indices"].append(i)
    
    bias_metrics = {}
    for group, data in groups.items():
        # Calculate fraud detection rate for this group
        fraud_keywords = ["suspicious", "high risk", "fraudulent", "flagged"]
        fraud_detections = sum(1 for pred in data["predictions"] 
                             if any(keyword in pred.lower() for keyword in fraud_keywords))
        detection_rate = fraud_detections / len(data["predictions"])
        
        bias_metrics[group] = {
            "detection_rate": detection_rate,
            "sample_size": len(data["predictions"]),
            "predictions": data["predictions"]
        }
    
    return bias_metrics

# Evaluate bias across different demographic attributes
demographic_attributes = ["age_group", "income_bracket", "location"]

print("\\nBias Assessment Results:")
for attribute in demographic_attributes:
    print(f"\\n{attribute.replace('_', ' ').title()} Bias Analysis:")
    bias_results = evaluate_demographic_bias(fraud_predictions, customer_demographics, attribute)
    
    detection_rates = [metrics["detection_rate"] for metrics in bias_results.values()]
    bias_variance = np.var(detection_rates)
    
    print(f"  Detection Rate Variance: {bias_variance:.4f}")
    
    for group, metrics in bias_results.items():
        print(f"    {group}: {metrics['detection_rate']:.3f} detection rate ({metrics['sample_size']} samples)")
    
    # Flag potential bias
    if bias_variance > 0.05:  # Threshold for concerning bias
        print(f"  ⚠️  HIGH BIAS DETECTED in {attribute}")
    else:
        print(f"  ✓ Acceptable bias levels in {attribute}")

# 3. Regulatory Compliance Monitoring
def generate_compliance_report(predictions, ground_truth, demographics):
    """Generate regulatory compliance report for financial AI"""
    
    # Calculate key compliance metrics
    total_predictions = len(predictions)
    
    # Accuracy metrics
    accuracy_metrics = client.compute_metrics(predictions, ground_truth)
    avg_accuracy = np.mean([np.mean(values) for values in accuracy_metrics.values()])
    
    # Bias metrics across protected classes
    bias_scores = {}
    for attribute in ["age_group", "income_bracket"]:
        bias_data = evaluate_demographic_bias(predictions, demographics, attribute)
        detection_rates = [data["detection_rate"] for data in bias_data.values()]
        bias_scores[attribute] = np.var(detection_rates)
    
    # False positive/negative analysis
    fraud_keywords = ["suspicious", "high risk", "fraudulent"]
    predicted_fraud = [any(keyword in pred.lower() for keyword in fraud_keywords) for pred in predictions]
    actual_fraud = [any(keyword in truth.lower() for keyword in fraud_keywords) for truth in ground_truth]
    
    true_positives = sum(1 for p, a in zip(predicted_fraud, actual_fraud) if p and a)
    false_positives = sum(1 for p, a in zip(predicted_fraud, actual_fraud) if p and not a)
    false_negatives = sum(1 for p, a in zip(predicted_fraud, actual_fraud) if not p and a)
    
    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
    
    return {
        "model_performance": {
            "overall_accuracy": avg_accuracy,
            "precision": precision,
            "recall": recall,
            "false_positive_rate": false_positives / total_predictions,
            "false_negative_rate": false_negatives / total_predictions,
        },
        "bias_assessment": {
            "age_bias_score": bias_scores.get("age_group", 0.0),
            "income_bias_score": bias_scores.get("income_bracket", 0.0),
            "bias_threshold_exceeded": any(score > 0.05 for score in bias_scores.values()),
        },
        "regulatory_compliance": {
            "fair_lending_compliant": bias_scores.get("income_bracket", 0.0) < 0.03,
            "age_discrimination_compliant": bias_scores.get("age_group", 0.0) < 0.03,
            "model_explainability": True,  # Assuming explanations are provided
            "audit_trail_complete": True,
        }
    }

compliance_report = generate_compliance_report(fraud_predictions, ground_truth, customer_demographics)

print("\\n=== Financial AI Compliance Report ===")
for category, metrics in compliance_report.items():
    print(f"\\n{category.replace('_', ' ').title()}:")
    for metric, value in metrics.items():
        if isinstance(value, bool):
            status = "✓ PASS" if value else "✗ FAIL"
            print(f"  {metric.replace('_', ' ').title()}: {status}")
        else:
            print(f"  {metric.replace('_', ' ').title()}: {value:.4f}")

# 4. Real-time Financial AI Monitoring
from blazemetrics import ProductionMonitor

financial_monitor = ProductionMonitor(
    models=["fraud-detector-v1", "credit-risk-model-v2"],
    metrics=["accuracy", "precision", "recall", "bias_score"],
    alert_thresholds={
        "accuracy": 0.90,      # High accuracy required for financial decisions
        "precision": 0.85,     # Minimize false positives (customer friction)
        "recall": 0.88,        # Minimize false negatives (missed fraud)
        "bias_score": 0.03,    # Very low bias tolerance for regulatory compliance
    }
)

def financial_alert_callback(result):
    """Handle financial AI alerts with regulatory considerations"""
    model = result.get('model', 'unknown')
    
    if result.get("bias_score", 0) > 0.03:
        print(f"🚨 REGULATORY ALERT: Bias threshold exceeded - {model}")
        print(f"   Bias Score: {result.get('bias_score', 0.0):.4f}")
        print("   Action: Immediate model review required for fair lending compliance")
    
    if result.get("precision", 1.0) < 0.85:
        print(f"⚠️  CUSTOMER IMPACT: High false positive rate - {model}")
        print(f"   Precision: {result.get('precision', 0.0):.3f}")
        print("   Action: Review model to reduce customer friction")
    
    if result.get("recall", 1.0) < 0.88:
        print(f"🚨 FRAUD RISK: High false negative rate - {model}")
        print(f"   Recall: {result.get('recall', 0.0):.3f}")
        print("   Action: Immediate investigation of missed fraud cases")

print("\\n=== Financial AI Production Monitoring ===")
print("Monitoring financial AI models for regulatory compliance...")

# Simulate monitoring (in practice, this would run continuously)
for step in range(3):
    result = {
        "model": f"fraud-detector-v{step+1}",
        "accuracy": 0.92 - (step * 0.02),
        "precision": 0.87 - (step * 0.01),
        "recall": 0.89 - (step * 0.015),
        "bias_score": 0.02 + (step * 0.01),
    }
    
    print(f"\\nStep {step+1}: {result['model']}")
    for metric, value in result.items():
        if metric != "model":
            print(f"  {metric}: {value:.3f}")
    
    financial_alert_callback(result)

print("\\n=== Financial AI Evaluation Complete ===")
print("All financial AI models evaluated for accuracy, bias, and regulatory compliance.")`}</code>
                </pre>
              </div>

              <div>
                <h4 className="font-semibold mb-2">Credit Risk Assessment Evaluation</h4>
                <pre className="text-sm overflow-x-auto bg-muted p-4 rounded-lg">
                  <code>{`# Credit risk model evaluation with fairness assessment
from blazemetrics import BlazeMetricsClient

# Credit risk assessments from AI model
credit_assessments = [
    "Approved: Strong credit history, stable income, low debt-to-income ratio.",
    "Denied: Insufficient credit history, high debt utilization, irregular income.",
    "Approved with conditions: Good credit score but recent late payments, higher interest rate recommended.",
    "Denied: Multiple recent credit inquiries, high existing debt burden.",
    "Approved: Excellent credit profile, long employment history, substantial assets.",
]

# Human underwriter decisions (ground truth)
underwriter_decisions = [
    "Approved: Excellent creditworthiness with strong financial indicators.",
    "Denied: High risk profile with insufficient credit history and income instability.",
    "Conditional approval: Generally good profile with minor recent credit issues.",
    "Denied: Overextended credit profile with multiple risk factors.",
    "Approved: Outstanding credit profile with minimal risk indicators.",
]

# Applicant demographics for fairness testing
applicant_demographics = [
    {"age": 35, "gender": "M", "ethnicity": "White", "income": 75000},
    {"age": 28, "gender": "F", "ethnicity": "Hispanic", "income": 45000},
    {"age": 42, "gender": "M", "ethnicity": "Black", "income": 65000},
    {"age": 55, "gender": "F", "ethnicity": "Asian", "income": 85000},
    {"age": 38, "gender": "M", "ethnicity": "White", "income": 95000},
]

client = BlazeMetricsClient(enable_analytics=True)

print("=== Credit Risk Assessment Evaluation ===")

# 1. Decision Accuracy Analysis
metrics = client.compute_metrics(credit_assessments, underwriter_decisions)
aggregated = client.aggregate_metrics(metrics)

print("\\nCredit Decision Accuracy:")
for metric, value in aggregated.items():
    print(f"  {metric}: {value:.3f}")

# 2. Fair Lending Analysis
def analyze_lending_fairness(assessments, demographics):
    """Analyze lending decisions for demographic fairness"""
    
    # Extract approval decisions
    approvals = []
    for assessment in assessments:
        if "approved" in assessment.lower():
            approvals.append(1)
        else:
            approvals.append(0)
    
    # Group by protected characteristics
    fairness_analysis = {}
    
    for protected_attr in ["gender", "ethnicity"]:
        groups = {}
        for i, demo in enumerate(demographics):
            group = demo[protected_attr]
            if group not in groups:
                groups[group] = {"approvals": [], "total": 0}
            groups[group]["approvals"].append(approvals[i])
            groups[group]["total"] += 1
        
        # Calculate approval rates by group
        group_rates = {}
        for group, data in groups.items():
            approval_rate = sum(data["approvals"]) / data["total"]
            group_rates[group] = {
                "approval_rate": approval_rate,
                "sample_size": data["total"]
            }
        
        fairness_analysis[protected_attr] = group_rates
    
    return fairness_analysis

fairness_results = analyze_lending_fairness(credit_assessments, applicant_demographics)

print("\\nFair Lending Analysis:")
for attribute, groups in fairness_results.items():
    print(f"\\n{attribute.title()} Group Analysis:")
    
    approval_rates = [data["approval_rate"] for data in groups.values()]
    rate_variance = np.var(approval_rates)
    
    for group, data in groups.items():
        print(f"  {group}: {data['approval_rate']:.3f} approval rate ({data['sample_size']} applicants)")
    
    print(f"  Approval Rate Variance: {rate_variance:.4f}")
    
    # Fair lending compliance check
    if rate_variance > 0.04:  # ECOA compliance threshold
        print(f"  ⚠️  POTENTIAL DISPARATE IMPACT in {attribute}")
    else:
        print(f"  ✓ Fair lending compliance for {attribute}")

# 3. Model Explainability Assessment
def assess_model_explainability(assessments):
    """Evaluate the explainability of credit decisions"""
    
    explanation_quality = []
    
    for assessment in assessments:
        # Check for key explanation components
        has_credit_history = "credit" in assessment.lower()
        has_income_info = "income" in assessment.lower()
        has_debt_info = "debt" in assessment.lower()
        has_specific_reasons = any(term in assessment.lower() 
                                 for term in ["ratio", "history", "score", "payments"])
        
        # Score explanation quality
        quality_score = sum([has_credit_history, has_income_info, has_debt_info, has_specific_reasons]) / 4
        explanation_quality.append(quality_score)
    
    return {
        "average_explanation_quality": np.mean(explanation_quality),
        "explanations_with_reasons": sum(1 for score in explanation_quality if score >= 0.5),
        "total_decisions": len(explanation_quality),
        "explainability_compliance": np.mean(explanation_quality) >= 0.6
    }

explainability_results = assess_model_explainability(credit_assessments)

print("\\n=== Model Explainability Assessment ===")
for metric, value in explainability_results.items():
    if isinstance(value, bool):
        status = "✓ COMPLIANT" if value else "✗ NON-COMPLIANT"
        print(f"  {metric.replace('_', ' ').title()}: {status}")
    elif isinstance(value, float):
        print(f"  {metric.replace('_', ' ').title()}: {value:.3f}")
    else:
        print(f"  {metric.replace('_', ' ').title()}: {value}")

# 4. Generate Regulatory Compliance Report
def generate_credit_compliance_report(assessments, demographics, fairness_results, explainability_results):
    """Generate comprehensive regulatory compliance report"""
    
    return {
        "fair_credit_reporting_act": {
            "adverse_action_notices": True,  # Assuming proper notices are sent
            "explanation_provided": explainability_results["explainability_compliance"],
            "accuracy_maintained": aggregated.get("bleu", 0.0) > 0.8,
        },
        "equal_credit_opportunity_act": {
            "gender_fairness": fairness_results["gender"],
            "ethnicity_fairness": fairness_results["ethnicity"],
            "disparate_impact_detected": any(
                np.var([data["approval_rate"] for data in groups.values()]) > 0.04
                for groups in fairness_results.values()
            ),
        },
        "model_governance": {
            "model_validation_complete": True,
            "bias_testing_performed": True,
            "explainability_standards_met": explainability_results["explainability_compliance"],
            "monitoring_in_place": True,
        }
    }

compliance_report = generate_credit_compliance_report(
    credit_assessments, 
    applicant_demographics, 
    fairness_results, 
    explainability_results
)

print("\\n=== Credit AI Regulatory Compliance Report ===")
for regulation, requirements in compliance_report.items():
    print(f"\\n{regulation.replace('_', ' ').title()}:")
    
    if isinstance(requirements, dict) and "gender_fairness" in requirements:
        # Special handling for fairness metrics
        print("  Demographic Fairness Assessment:")
        for demo_type, groups in requirements.items():
            if demo_type.endswith("_fairness"):
                demo_name = demo_type.replace("_fairness", "")
                print(f"    {demo_name.title()}:")
                for group, data in groups.items():
                    print(f"      {group}: {data['approval_rate']:.3f} approval rate")
    else:
        for requirement, status in requirements.items():
            if isinstance(status, bool):
                result = "✓ COMPLIANT" if status else "✗ NON-COMPLIANT"
                print(f"  {requirement.replace('_', ' ').title()}: {result}")
            else:
                print(f"  {requirement.replace('_', ' ').title()}: {status}")

print("\\n=== Credit Risk Evaluation Complete ===")
print("Credit risk model evaluated for accuracy, fairness, and regulatory compliance.")`}</code>
                </pre>
              </div>
            </CardContent>
          </Card>
        </TabsContent>

        <TabsContent value="customer" className="space-y-6">
          <Card>
            <CardHeader>
              <CardTitle className="flex items-center gap-2">
                <MessageSquare className="w-5 h-5 text-blue-500" />
                Customer Service AI Evaluation
              </CardTitle>
              <CardDescription>
                Comprehensive chatbot and customer support AI quality assessment with satisfaction prediction
              </CardDescription>
            </CardHeader>
            <CardContent className="space-y-4">
              <Alert>
                <CheckCircle className="h-4 w-4" />
                <AlertDescription>
                  Based on real backend examples: <code>01_llm_quality_and_analytics.py</code> and customer service evaluation patterns
                </AlertDescription>
              </Alert>

              <div>
                <h4 className="font-semibold mb-2">Customer Service Response Quality Evaluation</h4>
                <pre className="text-sm overflow-x-auto bg-muted p-4 rounded-lg">
                  <code>{`from blazemetrics import BlazeMetricsClient
from blazemetrics.llm_judge import LLMJudge
import openai

# Customer service chatbot responses
chatbot_responses = [
    "I understand you're having trouble with your order. Let me look that up for you right away. Can you please provide your order number?",
    "Sorry, I can't help with that. Please contact our billing department during business hours.",
    "Thank you for contacting us! I see you're asking about our return policy. You can return items within 30 days of purchase with original receipt.",
    "I apologize for the inconvenience. Let me escalate this to a human agent who can better assist you with this complex issue.",
    "Your account shows a pending charge. This typically processes within 2-3 business days. Is there anything else I can help you with today?",
]

# Ideal customer service responses (reference)
ideal_responses = [
    "I understand your concern about your order. I'll help you track it immediately. Please provide your order number so I can give you specific details.",
    "I'd be happy to help you with your billing question. Let me connect you with our billing specialist, or I can help you right now if you'd like.",
    "I'm glad to help with our return policy! You have 30 days from purchase to return items with receipt. Would you like me to start a return for you?",
    "I apologize for this issue. I'm escalating you to a specialist who can resolve this completely. They'll contact you within 2 hours.",
    "I can see the pending charge on your account. It will complete in 2-3 business days. I'll monitor this for you. Anything else I can help with?",
]

# Customer satisfaction scores (1-5 scale) for validation
satisfaction_scores = [4.2, 2.1, 4.5, 4.0, 3.8]

# Setup customer service evaluation
client = BlazeMetricsClient(
    enable_analytics=True,
    metrics_include=["bleu", "rouge1", "rouge2", "meteor", "chrf"]
)

# Setup LLM judge for customer service quality
judge = LLMJudge(
    provider="openai",
    api_key="your-openai-api-key",
    model="gpt-4o",
    system_prompt="""You are evaluating customer service responses. Rate each response on:
    - Helpfulness (0-1): How well does it address the customer's need?
    - Empathy (0-1): Does it show understanding and care for the customer?
    - Clarity (0-1): Is the response clear and easy to understand?
    - Professionalism (0-1): Is the tone appropriate and professional?
    - Actionability (0-1): Does it provide clear next steps or solutions?
    
    Return JSON with scores for each dimension and overall_quality (0-1)."""
)

print("=== Customer Service Response Quality Evaluation ===")

# 1. Basic Response Quality Metrics
metrics = client.compute_metrics(chatbot_responses, ideal_responses)
aggregated = client.aggregate_metrics(metrics)

print("\\nResponse Quality Metrics:")
for metric, value in aggregated.items():
    print(f"  {metric}: {value:.3f}")

# 2. Advanced Quality Assessment with LLM Judge
client.set_factuality_scorer(judge.score)
quality_results = client.evaluate_factuality(chatbot_responses, ideal_responses)

print("\\nAdvanced Quality Assessment:")
for i, result in enumerate(quality_results):
    print(f"\\nResponse {i+1}: {chatbot_responses[i][:60]}...")
    print(f"  Helpfulness: {result.get('helpfulness', 0.0):.3f}")
    print(f"  Empathy: {result.get('empathy', 0.0):.3f}")
    print(f"  Clarity: {result.get('clarity', 0.0):.3f}")
    print(f"  Professionalism: {result.get('professionalism', 0.0):.3f}")
    print(f"  Actionability: {result.get('actionability', 0.0):.3f}")
    print(f"  Overall Quality: {result.get('overall_quality', 0.0):.3f}")
    print(f"  Customer Satisfaction: {satisfaction_scores[i]:.1f}/5.0")

# 3. Intent Classification Accuracy
customer_intents = [
    "order_tracking",
    "billing_inquiry", 
    "return_policy",
    "technical_support",
    "account_status"
]

predicted_intents = [
    "order_tracking",
    "general_inquiry",  # Misclassified
    "return_policy",
    "escalation",       # Misclassified  
    "account_status"
]

def evaluate_intent_classification(true_intents, predicted_intents):
    """Evaluate intent classification accuracy"""
    correct = sum(1 for true, pred in zip(true_intents, predicted_intents) if true == pred)
    accuracy = correct / len(true_intents)
    
    # Calculate per-intent metrics
    intent_metrics = {}
    unique_intents = set(true_intents + predicted_intents)
    
    for intent in unique_intents:
        true_positives = sum(1 for t, p in zip(true_intents, predicted_intents) if t == intent and p == intent)
        false_positives = sum(1 for t, p in zip(true_intents, predicted_intents) if t != intent and p == intent)
        false_negatives = sum(1 for t, p in zip(true_intents, predicted_intents) if t == intent and p != intent)
        
        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        intent_metrics[intent] = {
            "precision": precision,
            "recall": recall,
            "f1_score": f1
        }
    
    return {
        "overall_accuracy": accuracy,
        "intent_metrics": intent_metrics
    }

intent_results = evaluate_intent_classification(customer_intents, predicted_intents)

print("\\n=== Intent Classification Evaluation ===")
print(f"Overall Accuracy: {intent_results['overall_accuracy']:.3f}")
print("\\nPer-Intent Metrics:")
for intent, metrics in intent_results["intent_metrics"].items():
    print(f"  {intent}:")
    print(f"    Precision: {metrics['precision']:.3f}")
    print(f"    Recall: {metrics['recall']:.3f}")
    print(f"    F1-Score: {metrics['f1_score']:.3f}")

# 4. Customer Satisfaction Prediction
import numpy as np
from sklearn.linear_model import LinearRegression

def predict_satisfaction_from_quality(quality_scores, actual_satisfaction):
    """Build model to predict satisfaction from quality metrics"""
    
    # Extract quality features
    features = []
    for result in quality_results:
        feature_vector = [
            result.get('helpfulness', 0.0),
            result.get('empathy', 0.0),
            result.get('clarity', 0.0),
            result.get('professionalism', 0.0),
            result.get('actionability', 0.0),
            result.get('overall_quality', 0.0)
        ]
        features.append(feature_vector)
    
    # Train simple satisfaction prediction model
    X = np.array(features)
    y = np.array(actual_satisfaction)
    
    model = LinearRegression()
    model.fit(X, y)
    
    # Predict satisfaction scores
    predicted_satisfaction = model.predict(X)
    
    # Calculate prediction accuracy
    mae = np.mean(np.abs(predicted_satisfaction - y))
    rmse = np.sqrt(np.mean((predicted_satisfaction - y) ** 2))
    
    return {
        "predicted_scores": predicted_satisfaction,
        "actual_scores": y,
        "mean_absolute_error": mae,
        "root_mean_square_error": rmse,
        "feature_importance": model.coef_
    }

satisfaction_prediction = predict_satisfaction_from_quality(quality_results, satisfaction_scores)

print("\\n=== Customer Satisfaction Prediction ===")
print(f"Mean Absolute Error: {satisfaction_prediction['mean_absolute_error']:.3f}")
print(f"Root Mean Square Error: {satisfaction_prediction['root_mean_square_error']:.3f}")

print("\\nPredicted vs Actual Satisfaction:")
for i, (pred, actual) in enumerate(zip(satisfaction_prediction['predicted_scores'], satisfaction_prediction['actual_scores'])):
    print(f"  Response {i+1}: Predicted={pred:.2f}, Actual={actual:.1f}")

print("\\nFeature Importance for Satisfaction:")
feature_names = ["Helpfulness", "Empathy", "Clarity", "Professionalism", "Actionability", "Overall Quality"]
for name, importance in zip(feature_names, satisfaction_prediction['feature_importance']):
    print(f"  {name}: {importance:.3f}")

# 5. Escalation Decision Quality
escalation_decisions = [
    {"should_escalate": False, "predicted_escalate": False, "reason": "Simple order tracking"},
    {"should_escalate": True, "predicted_escalate": False, "reason": "Complex billing dispute"},
    {"should_escalate": False, "predicted_escalate": False, "reason": "Standard return policy question"},
    {"should_escalate": True, "predicted_escalate": True, "reason": "Technical issue requiring specialist"},
    {"should_escalate": False, "predicted_escalate": False, "reason": "Account status inquiry"},
]

def evaluate_escalation_decisions(decisions):
    """Evaluate the quality of escalation decisions"""
    
    correct_decisions = sum(1 for d in decisions if d["should_escalate"] == d["predicted_escalate"])
    accuracy = correct_decisions / len(decisions)
    
    # Calculate escalation-specific metrics
    true_positives = sum(1 for d in decisions if d["should_escalate"] and d["predicted_escalate"])
    false_positives = sum(1 for d in decisions if not d["should_escalate"] and d["predicted_escalate"])
    false_negatives = sum(1 for d in decisions if d["should_escalate"] and not d["predicted_escalate"])
    
    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
    
    return {
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "missed_escalations": false_negatives,
        "unnecessary_escalations": false_positives
    }

escalation_results = evaluate_escalation_decisions(escalation_decisions)

print("\\n=== Escalation Decision Quality ===")
print(f"Decision Accuracy: {escalation_results['accuracy']:.3f}")
print(f"Escalation Precision: {escalation_results['precision']:.3f}")
print(f"Escalation Recall: {escalation_results['recall']:.3f}")
print(f"Missed Escalations: {escalation_results['missed_escalations']}")
print(f"Unnecessary Escalations: {escalation_results['unnecessary_escalations']}")

# 6. Multi-language Support Assessment
multilingual_responses = [
    {"language": "English", "response": "How can I help you today?", "quality_score": 0.95},
    {"language": "Spanish", "response": "¿Cómo puedo ayudarte hoy?", "quality_score": 0.88},
    {"language": "French", "response": "Comment puis-je vous aider aujourd'hui?", "quality_score": 0.82},
    {"language": "German", "response": "Wie kann ich Ihnen heute helfen?", "quality_score": 0.79},
    {"language": "Chinese", "response": "今天我可以如何帮助您？", "quality_score": 0.75},
]

print("\\n=== Multi-language Support Assessment ===")
avg_quality = np.mean([r["quality_score"] for r in multilingual_responses])
print(f"Average Multi-language Quality: {avg_quality:.3f}")

print("\\nPer-Language Quality:")
for response in multilingual_responses:
    print(f"  {response['language']}: {response['quality_score']:.3f}")

# Identify languages needing improvement
threshold = 0.85
needs_improvement = [r for r in multilingual_responses if r["quality_score"] < threshold]
if needs_improvement:
    print(f"\\nLanguages needing improvement (< {threshold}):")
    for lang in needs_improvement:
        print(f"  {lang['language']}: {lang['quality_score']:.3f}")

print("\\n=== Customer Service AI Evaluation Complete ===")
print("Comprehensive evaluation of response quality, intent classification, satisfaction prediction, and escalation decisions.")`}</code>
                </pre>
              </div>

              <div>
                <h4 className="font-semibold mb-2">Real-time Customer Service Monitoring</h4>
                <pre className="text-sm overflow-x-auto bg-muted p-4 rounded-lg">
                  <code>{`# Real-time monitoring for customer service AI systems
from blazemetrics import ProductionMonitor
import time

# Setup customer service monitoring
cs_monitor = ProductionMonitor(
    models=["chatbot-v1", "intent-classifier-v2", "escalation-predictor-v1"],
    metrics=["response_quality", "intent_accuracy", "satisfaction_score", "escalation_accuracy"],
    alert_thresholds={
        "response_quality": 0.75,      # Minimum acceptable response quality
        "intent_accuracy": 0.85,       # High accuracy needed for proper routing
        "satisfaction_score": 3.5,     # Target satisfaction above 3.5/5
        "escalation_accuracy": 0.80,   # Accurate escalation decisions
    }
)

def customer_service_alert_callback(result):
    """Handle customer service AI alerts"""
    model = result.get('model', 'unknown')
    
    if result.get("response_quality", 1.0) < 0.75:
        print(f"⚠️  QUALITY ALERT: Low response quality - {model}")
        print(f"   Quality Score: {result.get('response_quality', 0.0):.3f}")
        print("   Action: Review recent responses and retrain model")
    
    if result.get("satisfaction_score", 5.0) < 3.5:
        print(f"🚨 SATISFACTION ALERT: Low customer satisfaction - {model}")
        print(f"   Satisfaction: {result.get('satisfaction_score', 0.0):.1f}/5.0")
        print("   Action: Immediate review of customer interactions")
    
    if result.get("intent_accuracy", 1.0) < 0.85:
        print(f"⚠️  ROUTING ALERT: Poor intent classification - {model}")
        print(f"   Intent Accuracy: {result.get('intent_accuracy', 0.0):.3f}")
        print("   Action: Update intent classification training data")

print("\\n=== Customer Service AI Production Monitoring ===")

# Simulate real-time monitoring
for step in range(5):
    # Simulate varying performance metrics
    result = {
        "model": f"chatbot-v{(step % 3) + 1}",
        "response_quality": 0.85 - (step * 0.03),
        "intent_accuracy": 0.90 - (step * 0.02),
        "satisfaction_score": 4.2 - (step * 0.15),
        "escalation_accuracy": 0.82 - (step * 0.01),
        "timestamp": time.time()
    }
    
    print(f"\\nMonitoring Step {step+1}: {result['model']}")
    print(f"  Response Quality: {result['response_quality']:.3f}")
    print(f"  Intent Accuracy: {result['intent_accuracy']:.3f}")
    print(f"  Satisfaction Score: {result['satisfaction_score']:.1f}/5.0")
    print(f"  Escalation Accuracy: {result['escalation_accuracy']:.3f}")
    
    customer_service_alert_callback(result)
    
    time.sleep(0.5)

print("\\n=== Customer Service Monitoring Complete ===")
print("All customer service AI systems monitored for quality and customer satisfaction.")`}</code>
                </pre>
              </div>
            </CardContent>
          </Card>
        </TabsContent>

        <TabsContent value="content" className="space-y-6">
          <Card>
            <CardHeader>
              <CardTitle className="flex items-center gap-2">
                <FileText className="w-5 h-5 text-purple-500" />
                Content Generation Evaluation
              </CardTitle>
              <CardDescription>
                Comprehensive evaluation workflows for creative and technical content generation
              </CardDescription>
            </CardHeader>
            <CardContent className="space-y-4">
              <Alert>
                <CheckCircle className="h-4 w-4" />
                <AlertDescription>
                  Based on real backend examples: <code>01_llm_quality_and_analytics.py</code> and content evaluation patterns
                </AlertDescription>
              </Alert>

              <div>
                <h4 className="font-semibold mb-2">Creative Content Quality Assessment</h4>
                <pre className="text-sm overflow-x-auto bg-muted p-4 rounded-lg">
                  <code>{`from blazemetrics import BlazeMetricsClient
from blazemetrics.llm_judge import LLMJudge
import openai

# Generated content samples
generated_content = [
    """The morning sun cast golden rays across the bustling marketplace. Vendors called out their wares while customers weaved between colorful stalls, creating a symphony of commerce and community.""",
    
    """Our new product revolutionizes the way businesses handle customer data. With advanced encryption and intuitive interfaces, companies can now manage sensitive information with unprecedented security and ease.""",
    
    """Climate change represents one of the most pressing challenges of our time. Rising temperatures, melting ice caps, and extreme weather events demand immediate action from governments, businesses, and individuals worldwide.""",
    
    """The recipe for perfect chocolate chip cookies starts with room temperature butter. Cream it with brown and white sugars until fluffy, then add eggs and vanilla before incorporating the dry ingredients.""",
    
    """Machine learning algorithms have transformed modern healthcare. From diagnostic imaging to drug discovery, AI systems now assist doctors in making more accurate diagnoses and developing personalized treatment plans."""
]

# Reference content for comparison
reference_content = [
    """Golden sunlight illuminated the vibrant marketplace as vendors enthusiastically promoted their goods and shoppers navigated the maze of colorful booths, forming a harmonious blend of trade and community spirit.""",
    
    """This innovative product transforms business data management through cutting-edge encryption and user-friendly design, enabling organizations to handle confidential information with unmatched security and simplicity.""",
    
    """Climate change stands as humanity's greatest challenge today. Increasing global temperatures, shrinking polar ice, and severe weather patterns require urgent response from all sectors of society.""",
    
    """Perfect chocolate chip cookies begin with softened butter creamed with both sugars until light and airy, followed by eggs and vanilla, then carefully mixed with flour and other dry ingredients.""",
    
    """AI has revolutionized healthcare through machine learning applications in medical imaging, pharmaceutical research, and clinical decision support, helping physicians deliver more precise diagnoses and tailored treatments."""
]

# Content categories for specialized evaluation
content_categories = [
    "creative_writing",
    "marketing_copy", 
    "informational_article",
    "instructional_content",
    "technical_writing"
]

# Setup content evaluation
client = BlazeMetricsClient(
    enable_analytics=True,
    metrics_include=["bleu", "rouge1", "rouge2", "meteor", "chrf"]
)

# Setup specialized content judge
content_judge = LLMJudge(
    provider="openai",
    api_key="your-openai-api-key",
    model="gpt-4o",
    system_prompt="""Evaluate content quality across multiple dimensions:
    - Creativity (0-1): Originality and creative expression
    - Clarity (0-1): Clear communication and readability  
    - Engagement (0-1): Ability to capture and hold reader interest
    - Accuracy (0-1): Factual correctness and reliability
    - Style_consistency (0-1): Consistent tone and voice
    - SEO_potential (0-1): Search engine optimization potential
    
    Return JSON with scores for each dimension and overall_quality (0-1)."""
)

print("=== Content Generation Quality Evaluation ===")

# 1. Basic Content Quality Metrics
metrics = client.compute_metrics(generated_content, reference_content)
aggregated = client.aggregate_metrics(metrics)

print("\\nContent Quality Metrics:")
for metric, value in aggregated.items():
    print(f"  {metric}: {value:.3f}")

# 2. Advanced Content Assessment
client.set_factuality_scorer(content_judge.score)
quality_results = client.evaluate_factuality(generated_content, reference_content)

print("\\nAdvanced Content Quality Assessment:")
for i, result in enumerate(quality_results):
    category = content_categories[i]
    print(f"\\n{category.replace('_', ' ').title()}:")
    print(f"  Content: {generated_content[i][:80]}...")
    print(f"  Creativity: {result.get('creativity', 0.0):.3f}")
    print(f"  Clarity: {result.get('clarity', 0.0):.3f}")
    print(f"  Engagement: {result.get('engagement', 0.0):.3f}")
    print(f"  Accuracy: {result.get('accuracy', 0.0):.3f}")
    print(f"  Style Consistency: {result.get('style_consistency', 0.0):.3f}")
    print(f"  SEO Potential: {result.get('seo_potential', 0.0):.3f}")
    print(f"  Overall Quality: {result.get('overall_quality', 0.0):.3f}")

# 3. Brand Voice Consistency Analysis
brand_guidelines = {
    "tone": "professional yet approachable",
    "voice": "confident and helpful",
    "style": "clear and concise",
    "avoid": ["jargon", "overly technical language", "passive voice"]
}

def evaluate_brand_consistency(content, guidelines):
    """Evaluate content adherence to brand guidelines"""
    
    consistency_scores = []
    
    for text in content:
        # Simple heuristic-based evaluation (in practice, use more sophisticated NLP)
        score = 0.0
        
        # Check for professional tone indicators
        professional_indicators = ["innovative", "advanced", "comprehensive", "effective"]
        if any(indicator in text.lower() for indicator in professional_indicators):
            score += 0.25
        
        # Check for approachable language
        approachable_indicators = ["easy", "simple", "help", "support"]
        if any(indicator in text.lower() for indicator in approachable_indicators):
            score += 0.25
        
        # Check for clarity (sentence length)
        sentences = text.split('.')
        avg_sentence_length = sum(len(s.split()) for s in sentences if s.strip()) / len([s for s in sentences if s.strip()])
        if avg_sentence_length < 25:  # Reasonable sentence length
            score += 0.25
        
        # Check for avoided terms
        avoided_terms = guidelines["avoid"]
        if not any(term in text.lower() for term in avoided_terms):
            score += 0.25
        
        consistency_scores.append(score)
    
    return consistency_scores

brand_consistency = evaluate_brand_consistency(generated_content, brand_guidelines)

print("\\n=== Brand Voice Consistency Analysis ===")
avg_consistency = sum(brand_consistency) / len(brand_consistency)
print(f"Average Brand Consistency: {avg_consistency:.3f}")

print("\\nPer-Content Brand Consistency:")
for i, (score, category) in enumerate(zip(brand_consistency, content_categories)):
    status = "✓" if score >= 0.75 else "⚠️" if score >= 0.5 else "✗"
    print(f"  {category.replace('_', ' ').title()}: {score:.3f} {status}")

# 4. SEO Optimization Assessment
def evaluate_seo_potential(content):
    """Evaluate SEO optimization potential of content"""
    
    seo_scores = []
    
    for text in content:
        score = 0.0
        word_count = len(text.split())
        
        # Optimal length check (300-2000 words for most content)
        if 50 <= word_count <= 500:  # Adjusted for shorter samples
            score += 0.2
        
        # Keyword density (simple check for repeated important terms)
        words = text.lower().split()
        word_freq = {}
        for word in words:
            if len(word) > 4:  # Focus on meaningful words
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # Check for reasonable keyword repetition
        max_freq = max(word_freq.values()) if word_freq else 0
        if 1 <= max_freq <= 3:  # Good keyword density
            score += 0.2
        
        # Readability (sentence structure)
        sentences = [s.strip() for s in text.split('.') if s.strip()]
        if sentences:
            avg_words_per_sentence = word_count / len(sentences)
            if 10 <= avg_words_per_sentence <= 20:  # Good readability
                score += 0.2
        
        # Content structure (presence of descriptive language)
        descriptive_words = ["advanced", "innovative", "comprehensive", "effective", "perfect", "revolutionary"]
        if any(word in text.lower() for word in descriptive_words):
            score += 0.2
        
        # Engagement factors (questions, calls to action)
        engagement_indicators = ["?", "now", "today", "discover", "learn"]
        if any(indicator in text.lower() for indicator in engagement_indicators):
            score += 0.2
        
        seo_scores.append(score)
    
    return seo_scores

seo_scores = evaluate_seo_potential(generated_content)

print("\\n=== SEO Optimization Assessment ===")
avg_seo = sum(seo_scores) / len(seo_scores)
print(f"Average SEO Score: {avg_seo:.3f}")

print("\\nPer-Content SEO Analysis:")
for i, (score, category) in enumerate(zip(seo_scores, content_categories)):
    optimization_level = "Excellent" if score >= 0.8 else "Good" if score >= 0.6 else "Needs Improvement"
    print(f"  {category.replace('_', ' ').title()}: {score:.3f} ({optimization_level})")

# 5. Plagiarism and Originality Check
def check_originality(generated, reference):
    """Simple originality check using similarity metrics"""
    
    originality_scores = []
    
    for gen_text, ref_text in zip(generated, reference):
        # Use BLEU score as inverse originality measure
        bleu_metrics = client.compute_metrics([gen_text], [[ref_text]])
        bleu_score = bleu_metrics["bleu"][0]
        
        # Higher BLEU means less original (more similar to reference)
        originality = 1.0 - bleu_score
        originality_scores.append(originality)
    
    return originality_scores

originality_scores = check_originality(generated_content, reference_content)

print("\\n=== Originality and Plagiarism Check ===")
avg_originality = sum(originality_scores) / len(originality_scores)
print(f"Average Originality Score: {avg_originality:.3f}")

print("\\nPer-Content Originality:")
for i, (score, category) in enumerate(zip(originality_scores, content_categories)):
    originality_level = "Highly Original" if score >= 0.8 else "Moderately Original" if score >= 0.6 else "Low Originality"
    print(f"  {category.replace('_', ' ').title()}: {score:.3f} ({originality_level})")

# 6. Content Performance Prediction
import numpy as np

def predict_content_performance(quality_results, brand_scores, seo_scores, originality_scores):
    """Predict content performance based on quality metrics"""
    
    performance_predictions = []
    
    for i in range(len(quality_results)):
        # Weighted performance score
        weights = {
            "quality": 0.3,
            "brand_consistency": 0.2,
            "seo_potential": 0.25,
            "originality": 0.25
        }
        
        performance_score = (
            quality_results[i].get('overall_quality', 0.0) * weights["quality"] +
            brand_scores[i] * weights["brand_consistency"] +
            seo_scores[i] * weights["seo_potential"] +
            originality_scores[i] * weights["originality"]
        )
        
        # Predict engagement metrics
        predicted_engagement = {
            "performance_score": performance_score,
            "expected_reach": int(performance_score * 10000),  # Simulated reach
            "engagement_rate": performance_score * 0.05,       # Simulated engagement rate
            "conversion_potential": performance_score * 0.03   # Simulated conversion rate
        }
        
        performance_predictions.append(predicted_engagement)
    
    return performance_predictions

performance_predictions = predict_content_performance(
    quality_results, 
    brand_consistency, 
    seo_scores, 
    originality_scores
)

print("\\n=== Content Performance Predictions ===")
for i, (prediction, category) in enumerate(zip(performance_predictions, content_categories)):
    print(f"\\n{category.replace('_', ' ').title()}:")
    print(f"  Performance Score: {prediction['performance_score']:.3f}")
    print(f"  Expected Reach: {prediction['expected_reach']:,} users")
    print(f"  Engagement Rate: {prediction['engagement_rate']:.1%}")
    print(f"  Conversion Potential: {prediction['conversion_potential']:.1%}")

print("\\n=== Content Generation Evaluation Complete ===")
print("Comprehensive evaluation of content quality, brand consistency, SEO potential, and performance predictions.")`}</code>
                </pre>
              </div>

              <div>
                <h4 className="font-semibold mb-2">Content A/B Testing and Optimization</h4>
                <pre className="text-sm overflow-x-auto bg-muted p-4 rounded-lg">
                  <code>{`# A/B testing framework for content optimization
from blazemetrics import BlazeMetricsClient
import numpy as np

# A/B test variants for marketing copy
variant_a = [
    "Discover the future of AI with our revolutionary platform.",
    "Transform your business with cutting-edge technology.",
    "Join thousands of satisfied customers today."
]

variant_b = [
    "Unlock AI's potential with our game-changing solution.",
    "Revolutionize your operations with advanced AI tools.", 
    "Experience the difference - start your free trial now."
]

# Simulated performance metrics
performance_metrics = {
    "variant_a": {
        "click_through_rate": [0.032, 0.028, 0.035],
        "conversion_rate": [0.024, 0.019, 0.027],
        "engagement_time": [45, 38, 52],  # seconds
        "bounce_rate": [0.65, 0.72, 0.58]
    },
    "variant_b": {
        "click_through_rate": [0.041, 0.038, 0.044],
        "conversion_rate": [0.031, 0.029, 0.034],
        "engagement_time": [58, 62, 55],  # seconds
        "bounce_rate": [0.52, 0.48, 0.55]
    }
}

client = BlazeMetricsClient(enable_analytics=True)

print("=== Content A/B Testing Analysis ===")

# 1. Content Quality Comparison
print("\\nContent Quality Comparison:")
quality_a = client.compute_metrics(variant_a, variant_a)  # Self-comparison for baseline
quality_b = client.compute_metrics(variant_b, variant_b)

# Compare variants directly
cross_comparison = client.compute_metrics(variant_a, variant_b)
avg_similarity = np.mean([np.mean(values) for values in cross_comparison.values()])

print(f"Content Similarity Between Variants: {avg_similarity:.3f}")
print("(Lower similarity indicates more diverse testing)")

# 2. Performance Analysis
def analyze_ab_performance(metrics_a, metrics_b):
    """Analyze A/B test performance differences"""
    
    results = {}
    
    for metric in metrics_a.keys():
        a_values = np.array(metrics_a[metric])
        b_values = np.array(metrics_b[metric])
        
        # Calculate statistical significance (simplified t-test)
        a_mean, b_mean = np.mean(a_values), np.mean(b_values)
        a_std, b_std = np.std(a_values), np.std(b_values)
        
        # Effect size (Cohen's d)
        pooled_std = np.sqrt(((len(a_values) - 1) * a_std**2 + (len(b_values) - 1) * b_std**2) / 
                           (len(a_values) + len(b_values) - 2))
        cohens_d = (b_mean - a_mean) / pooled_std if pooled_std > 0 else 0
        
        # Percent improvement
        improvement = ((b_mean - a_mean) / a_mean * 100) if a_mean > 0 else 0
        
        results[metric] = {
            "variant_a_mean": a_mean,
            "variant_b_mean": b_mean,
            "improvement_percent": improvement,
            "effect_size": cohens_d,
            "significant": abs(cohens_d) > 0.5  # Medium effect size threshold
        }
    
    return results

ab_results = analyze_ab_performance(performance_metrics["variant_a"], performance_metrics["variant_b"])

print("\\nA/B Test Performance Results:")
for metric, results in ab_results.items():
    print(f"\\n{metric.replace('_', ' ').title()}:")
    print(f"  Variant A: {results['variant_a_mean']:.4f}")
    print(f"  Variant B: {results['variant_b_mean']:.4f}")
    print(f"  Improvement: {results['improvement_percent']:+.1f}%")
    print(f"  Effect Size: {results['effect_size']:.3f}")
    print(f"  Significant: {'✓' if results['significant'] else '✗'}")

# 3. Winner Determination
def determine_winner(ab_results):
    """Determine overall A/B test winner"""
    
    # Weight metrics by business importance
    metric_weights = {
        "click_through_rate": 0.3,
        "conversion_rate": 0.4,
        "engagement_time": 0.2,
        "bounce_rate": -0.1  # Negative weight (lower is better)
    }
    
    weighted_score = 0
    total_weight = 0
    
    for metric, weight in metric_weights.items():
        if metric in ab_results:
            improvement = ab_results[metric]["improvement_percent"] / 100
            weighted_score += improvement * weight
            total_weight += abs(weight)
    
    overall_improvement = weighted_score / total_weight if total_weight > 0 else 0
    
    return {
        "winner": "Variant B" if overall_improvement > 0 else "Variant A",
        "overall_improvement": overall_improvement * 100,
        "confidence": "High" if abs(overall_improvement) > 0.1 else "Medium" if abs(overall_improvement) > 0.05 else "Low"
    }

winner_analysis = determine_winner(ab_results)

print("\\n=== A/B Test Winner Analysis ===")
print(f"Winner: {winner_analysis['winner']}")
print(f"Overall Improvement: {winner_analysis['overall_improvement']:+.1f}%")
print(f"Confidence Level: {winner_analysis['confidence']}")

# 4. Content Optimization Recommendations
def generate_optimization_recommendations(variant_a, variant_b, ab_results, winner):
    """Generate content optimization recommendations"""
    
    recommendations = []
    
    # Analyze winning elements
    if winner == "Variant B":
        winning_content = variant_b
        losing_content = variant_a
    else:
        winning_content = variant_a
        losing_content = variant_b
    
    # Identify successful patterns
    winning_words = set()
    for content in winning_content:
        words = content.lower().split()
        winning_words.update(words)
    
    losing_words = set()
    for content in losing_content:
        words = content.lower().split()
        losing_words.update(words)
    
    # Find unique winning elements
    unique_winning = winning_words - losing_words
    
    recommendations.append({
        "category": "Vocabulary",
        "recommendation": f"Use words like: {', '.join(list(unique_winning)[:5])}",
        "impact": "Medium"
    })
    
    # Analyze performance metrics
    if ab_results["conversion_rate"]["improvement_percent"] > 10:
        recommendations.append({
            "category": "Call-to-Action",
            "recommendation": "Current CTA strategy shows strong conversion improvement",
            "impact": "High"
        })
    
    if ab_results["engagement_time"]["improvement_percent"] > 15:
        recommendations.append({
            "category": "Content Structure",
            "recommendation": "Content format effectively increases user engagement",
            "impact": "High"
        })
    
    if ab_results["bounce_rate"]["improvement_percent"] < -10:  # Negative is good for bounce rate
        recommendations.append({
            "category": "Content Relevance",
            "recommendation": "Content better matches user intent and expectations",
            "impact": "Medium"
        })
    
    return recommendations

optimization_recommendations = generate_optimization_recommendations(
    variant_a, variant_b, ab_results, winner_analysis["winner"]
)

print("\\n=== Content Optimization Recommendations ===")
for rec in optimization_recommendations:
    print(f"\\n{rec['category']} (Impact: {rec['impact']}):")
    print(f"  {rec['recommendation']}")

# 5. Future Test Planning
def plan_next_tests(current_results, winning_content):
    """Plan next round of A/B tests based on current results"""
    
    next_tests = []
    
    # If conversion rate improvement was significant, test more aggressive CTAs
    if current_results["conversion_rate"]["improvement_percent"] > 15:
        next_tests.append({
            "test_type": "CTA Optimization",
            "hypothesis": "More urgent/specific CTAs will further improve conversions",
            "variants": ["pip install blazemetrics Today", "Get Instant Access Now", "Join 10,000+ Users"]
        })
    
    # If engagement time improved, test longer content
    if current_results["engagement_time"]["improvement_percent"] > 20:
        next_tests.append({
            "test_type": "Content Length",
            "hypothesis": "Longer, more detailed content will maintain high engagement",
            "variants": ["Short form", "Medium form", "Long form with examples"]
        })
    
    # If click-through rate improved, test visual elements
    if current_results["click_through_rate"]["improvement_percent"] > 10:
        next_tests.append({
            "test_type": "Visual Elements",
            "hypothesis": "Adding visual cues will further improve click-through rates",
            "variants": ["Text only", "With icons", "With images"]
        })
    
    return next_tests

next_test_plan = plan_next_tests(ab_results, variant_b if winner_analysis["winner"] == "Variant B" else variant_a)

print("\\n=== Next A/B Test Planning ===")
for test in next_test_plan:
    print(f"\\n{test['test_type']}:")
    print(f"  Hypothesis: {test['hypothesis']}")
    print(f"  Test Variants: {', '.join(test['variants'])}")

print("\\n=== Content A/B Testing Complete ===")
print("Comprehensive A/B test analysis with winner determination and optimization recommendations.")`}</code>
                </pre>
              </div>
            </CardContent>
          </Card>
        </TabsContent>

        <TabsContent value="code" className="space-y-6">
          <Card>
            <CardHeader>
              <CardTitle className="flex items-center gap-2">
                <Code className="w-5 h-5 text-orange-500" />
                Code Generation Evaluation
              </CardTitle>
              <CardDescription>
                Comprehensive code evaluation workflows for correctness, security, and performance
              </CardDescription>
            </CardHeader>
            <CardContent className="space-y-4">
              <Alert>
                <CheckCircle className="h-4 w-4" />
                <AlertDescription>
                  Based on real backend examples: <code>06_code_generation_evaluation.py</code> and <code>09_code_security_eval.py</code>
                </AlertDescription>
              </Alert>

              <div>
                <h4 className="font-semibold mb-2">Multi-Language Code Evaluation</h4>
                <pre className="text-sm overflow-x-auto bg-muted p-4 rounded-lg">
                  <code>{`from blazemetrics import CodeEvaluator
from blazemetrics import BlazeMetricsClient

# Code generation prompts
prompts = [
    "Write a function to calculate the factorial of a number in Python",
    "Create a binary search algorithm in Java",
    "Implement a REST API endpoint in Node.js for user authentication",
    "Write a SQL query to find the top 5 customers by total orders",
    "Create a React component for a user profile form"
]

# Generated code samples
generated_code = [
    # Python factorial
    """def factorial(n):
    if n < 0:
        return None
    elif n == 0 or n == 1:
        return 1
    else:
        return n * factorial(n - 1)""",
    
    # Java binary search
    """public static int binarySearch(int[] arr, int target) {
    int left = 0, right = arr.length - 1;
    
    while (left <= right) {
        int mid = left + (right - left) / 2;
        
        if (arr[mid] == target) {
            return mid;
        } else if (arr[mid] < target) {
            left = mid + 1;
        } else {
            right = mid - 1;
        }
    }
    return -1;
}""",
    
    # Node.js authentication endpoint
    """app.post('/auth/login', async (req, res) => {
    try {
        const { email, password } = req.body;
        
        // Validate input
        if (!email || !password) {
            return res.status(400).json({ error: 'Email and password required' });
        }
        
        // Find user
        const user = await User.findOne({ email });
        if (!user) {
            return res.status(401).json({ error: 'Invalid credentials' });
        }
        
        // Verify password
        const isValid = await bcrypt.compare(password, user.password);
        if (!isValid) {
            return res.status(401).json({ error: 'Invalid credentials' });
        }
        
        // Generate JWT
        const token = jwt.sign({ userId: user._id }, process.env.JWT_SECRET);
        res.json({ token, user: { id: user._id, email: user.email } });
        
    } catch (error) {
        res.status(500).json({ error: 'Server error' });
    }
});""",
    
    # SQL query
    """SELECT 
    c.customer_id,
    c.customer_name,
    c.email,
    COUNT(o.order_id) as total_orders,
    SUM(o.total_amount) as total_spent
FROM customers c
JOIN orders o ON c.customer_id = o.customer_id
GROUP BY c.customer_id, c.customer_name, c.email
ORDER BY total_orders DESC
LIMIT 5;""",
    
    # React component
    """import React, { useState } from 'react';

const UserProfileForm = ({ user, onSubmit }) => {
    const [formData, setFormData] = useState({
        name: user?.name || '',
        email: user?.email || '',
        bio: user?.bio || ''
    });
    
    const handleChange = (e) => {
        setFormData({
            ...formData,
            [e.target.name]: e.target.value
        });
    };
    
    const handleSubmit = (e) => {
        e.preventDefault();
        onSubmit(formData);
    };
    
    return (
        <form onSubmit={handleSubmit}>
            <div>
                <label htmlFor="name">Name:</label>
                <input
                    type="text"
                    id="name"
                    name="name"
                    value={formData.name}
                    onChange={handleChange}
                    required
                />
            </div>
            <div>
                <label htmlFor="email">Email:</label>
                <input
                    type="email"
                    id="email"
                    name="email"
                    value={formData.email}
                    onChange={handleChange}
                    required
                />
            </div>
            <div>
                <label htmlFor="bio">Bio:</label>
                <textarea
                    id="bio"
                    name="bio"
                    value={formData.bio}
                    onChange={handleChange}
                />
            </div>
            <button type="submit">Update Profile</button>
        </form>
    );
};

export default UserProfileForm;"""
]

# Reference solutions
reference_solutions = [
    # Python factorial reference
    """def factorial(n):
    if n < 0:
        raise ValueError("Factorial not defined for negative numbers")
    if n == 0 or n == 1:
        return 1
    return n * factorial(n - 1)""",
    
    # Java binary search reference
    """public static int binarySearch(int[] arr, int target) {
    int left = 0;
    int right = arr.length - 1;
    
    while (left <= right) {
        int mid = left + (right - left) / 2;
        
        if (arr[mid] == target) {
            return mid;
        }
        
        if (arr[mid] < target) {
            left = mid + 1;
        } else {
            right = mid - 1;
        }
    }
    
    return -1;
}""",
    
    # Node.js reference
    """app.post('/auth/login', async (req, res) => {
    try {
        const { email, password } = req.body;
        
        if (!email || !password) {
            return res.status(400).json({ error: 'Email and password are required' });
        }
        
        const user = await User.findOne({ email });
        if (!user || !await bcrypt.compare(password, user.password)) {
            return res.status(401).json({ error: 'Invalid credentials' });
        }
        
        const token = jwt.sign({ userId: user._id }, process.env.JWT_SECRET, { expiresIn: '24h' });
        res.json({ token, user: { id: user._id, email: user.email } });
        
    } catch (error) {
        console.error('Login error:', error);
        res.status(500).json({ error: 'Internal server error' });
    }
});""",
    
    # SQL reference
    """SELECT 
    c.customer_id,
    c.customer_name,
    c.email,
    COUNT(o.order_id) as total_orders,
    SUM(o.total_amount) as total_spent
FROM customers c
INNER JOIN orders o ON c.customer_id = o.customer_id
GROUP BY c.customer_id, c.customer_name, c.email
ORDER BY total_orders DESC, total_spent DESC
LIMIT 5;""",
    
    # React reference
    """import React, { useState } from 'react';

const UserProfileForm = ({ user, onSubmit }) => {
    const [formData, setFormData] = useState({
        name: user?.name || '',
        email: user?.email || '',
        bio: user?.bio || ''
    });
    
    const [errors, setErrors] = useState({});
    
    const handleChange = (e) => {
        const { name, value } = e.target;
        setFormData(prev => ({
            ...prev,
            [name]: value
        }));
        
        // Clear error when user starts typing
        if (errors[name]) {
            setErrors(prev => ({
                ...prev,
                [name]: ''
            }));
        }
    };
    
    const validateForm = () => {
        const newErrors = {};
        
        if (!formData.name.trim()) {
            newErrors.name = 'Name is required';
        }
        
        if (!formData.email.trim()) {
            newErrors.email = 'Email is required';
        } else if (!/\\S+@\\S+\\.\\S+/.test(formData.email)) {
            newErrors.email = 'Email is invalid';
        }
        
        setErrors(newErrors);
        return Object.keys(newErrors).length === 0;
    };
    
    const handleSubmit = (e) => {
        e.preventDefault();
        
        if (validateForm()) {
            onSubmit(formData);
        }
    };
    
    return (
        <form onSubmit={handleSubmit} className="user-profile-form">
            <div className="form-group">
                <label htmlFor="name">Name *</label>
                <input
                    type="text"
                    id="name"
                    name="name"
                    value={formData.name}
                    onChange={handleChange}
                    className={errors.name ? 'error' : ''}
                />
                {errors.name && <span className="error-message">{errors.name}</span>}
            </div>
            
            <div className="form-group">
                <label htmlFor="email">Email *</label>
                <input
                    type="email"
                    id="email"
                    name="email"
                    value={formData.email}
                    onChange={handleChange}
                    className={errors.email ? 'error' : ''}
                />
                {errors.email && <span className="error-message">{errors.email}</span>}
            </div>
            
            <div className="form-group">
                <label htmlFor="bio">Bio</label>
                <textarea
                    id="bio"
                    name="bio"
                    value={formData.bio}
                    onChange={handleChange}
                    rows={4}
                />
            </div>
            
            <button type="submit" className="submit-button">
                Update Profile
            </button>
        </form>
    );
};

export default UserProfileForm;"""
]

# Programming languages
languages = ["python", "java", "javascript", "sql", "javascript"]

# Setup code evaluator
code_evaluator = CodeEvaluator(
    languages=languages,
    security_checks=True,
    performance_analysis=True,
    style_checks=True
)

print("=== Multi-Language Code Generation Evaluation ===")

# 1. Basic Code Quality Metrics
results = code_evaluator.evaluate(prompts, generated_code, reference_solutions)

print("\\nCode Quality Metrics:")
for metric, value in results.items():
    print(f"  {metric}: {value:.3f}")

# 2. Language-Specific Analysis
language_results = {}
for i, (lang, gen_code, ref_code) in enumerate(zip(languages, generated_code, reference_solutions)):
    lang_evaluator = CodeEvaluator(languages=[lang], security_checks=True)
    lang_result = lang_evaluator.evaluate([prompts[i]], [gen_code], [ref_code])
    language_results[f"{lang}_{i}"] = lang_result

print("\\nPer-Language Analysis:")
for lang_key, metrics in language_results.items():
    lang_name = lang_key.split('_')[0]
    index = int(lang_key.split('_')[1])
    print(f"\\n{lang_name.title()} - {prompts[index][:50]}...")
    for metric, value in metrics.items():
        print(f"  {metric}: {value:.3f}")

# 3. Security Vulnerability Assessment
def assess_security_vulnerabilities(code_samples, languages):
    """Assess security vulnerabilities in generated code"""
    
    security_issues = []
    
    for i, (code, lang) in enumerate(zip(code_samples, languages)):
        issues = []
        
        if lang == "javascript" and "app.post" in code:
            # Check for common Node.js security issues
            if "bcrypt.compare" not in code:
                issues.append("Missing password hashing verification")
            if "process.env.JWT_SECRET" not in code:
                issues.append("Hardcoded JWT secret")
            if "expiresIn" not in code:
                issues.append("JWT token without expiration")
            if "console.error" not in code:
                issues.append("Missing error logging")
        
        elif lang == "sql":
            # Check for SQL injection vulnerabilities
            if "'" in code and "CONCAT" in code.upper():
                issues.append("Potential SQL injection vulnerability")
            if "DROP" in code.upper() or "DELETE" in code.upper():
                issues.append("Potentially dangerous SQL operations")
        
        elif lang == "python":
            # Check for Python security issues
            if "eval(" in code or "exec(" in code:
                issues.append("Use of dangerous eval/exec functions")
            if "input(" in code and "int(" not in code:
                issues.append("Unvalidated user input")
        
        security_issues.append({
            "language": lang,
            "prompt": prompts[i][:50] + "...",
            "issues": issues,
            "security_score": max(0, 1.0 - len(issues) * 0.2)
        })
    
    return security_issues

security_assessment = assess_security_vulnerabilities(generated_code, languages)

print("\\n=== Security Vulnerability Assessment ===")
for assessment in security_assessment:
    print(f"\\n{assessment['language'].title()}: {assessment['prompt']}")
    print(f"  Security Score: {assessment['security_score']:.3f}")
    if assessment['issues']:
        print("  Security Issues:")
        for issue in assessment['issues']:
            print(f"    • {issue}")
    else:
        print("  ✓ No major security issues detected")

# 4. Performance Analysis
def analyze_code_performance(code_samples, languages):
    """Analyze performance characteristics of generated code"""
    
    performance_analysis = []
    
    for i, (code, lang) in enumerate(zip(code_samples, languages)):
        analysis = {
            "language": lang,
            "prompt": prompts[i][:50] + "...",
            "time_complexity": "Unknown",
            "space_complexity": "Unknown",
            "performance_score": 0.8,  # Default score
            "optimizations": []
        }
        
        if "factorial" in prompts[i].lower():
            if "return n * factorial(n - 1)" in code:
                analysis["time_complexity"] = "O(n)"
                analysis["space_complexity"] = "O(n)"
                analysis["optimizations"].append("Consider iterative approach to reduce space complexity")
            
        elif "binary search" in prompts[i].lower():
            if "while" in code and "left <= right" in code:
                analysis["time_complexity"] = "O(log n)"
                analysis["space_complexity"] = "O(1)"
                analysis["performance_score"] = 0.95
            
        elif "sql" in lang.lower():
            if "JOIN" in code.upper():
                analysis["performance_score"] = 0.85
                if "INDEX" not in code.upper():
                    analysis["optimizations"].append("Consider adding indexes on join columns")
            
        elif "react" in prompts[i].lower():
            if "useState" in code:
                analysis["performance_score"] = 0.8
                if "useCallback" not in code and "useMemo" not in code:
                    analysis["optimizations"].append("Consider memoization for performance optimization")
        
        performance_analysis.append(analysis)
    
    return performance_analysis

performance_results = analyze_code_performance(generated_code, languages)

print("\\n=== Performance Analysis ===")
for analysis in performance_results:
    print(f"\\n{analysis['language'].title()}: {analysis['prompt']}")
    print(f"  Time Complexity: {analysis['time_complexity']}")
    print(f"  Space Complexity: {analysis['space_complexity']}")
    print(f"  Performance Score: {analysis['performance_score']:.3f}")
    if analysis['optimizations']:
        print("  Optimization Suggestions:")
        for opt in analysis['optimizations']:
            print(f"    • {opt}")

# 5. Code Style and Best Practices
def evaluate_code_style(code_samples, languages):
    """Evaluate code style and best practices adherence"""
    
    style_evaluations = []
    
    for i, (code, lang) in enumerate(zip(code_samples, languages)):
        evaluation = {
            "language": lang,
            "prompt": prompts[i][:50] + "...",
            "style_score": 0.8,
            "issues": [],
            "best_practices": []
        }
        
        if lang == "python":
            # Check Python style
            if "def " in code and code.count('\\n') > 0:
                evaluation["best_practices"].append("✓ Function definition follows PEP 8")
            if "    " in code:  # Check for proper indentation
                evaluation["best_practices"].append("✓ Proper indentation used")
            if code.count('\\n') < 3:
                evaluation["issues"].append("Function could benefit from docstring")
        
        elif lang == "java":
            # Check Java style
            if "public static" in code:
                evaluation["best_practices"].append("✓ Proper access modifiers used")
            if "{" in code and "}" in code:
                evaluation["best_practices"].append("✓ Proper brace usage")
            if "camelCase" in str([word for word in code.split() if any(c.isupper() for c in word[1:])]):
                evaluation["best_practices"].append("✓ CamelCase naming convention")
        
        elif lang == "javascript":
            # Check JavaScript style
            if "const " in code or "let " in code:
                evaluation["best_practices"].append("✓ Modern variable declarations")
            if "async" in code and "await" in code:
                evaluation["best_practices"].append("✓ Proper async/await usage")
            if "try {" in code and "catch" in code:
                evaluation["best_practices"].append("✓ Error handling implemented")
        
        # Calculate style score based on issues and best practices
        style_score = 0.6 + (len(evaluation["best_practices"]) * 0.1) - (len(evaluation["issues"]) * 0.1)
        evaluation["style_score"] = min(1.0, max(0.0, style_score))
        
        style_evaluations.append(evaluation)
    
    return style_evaluations

style_results = evaluate_code_style(generated_code, languages)

print("\\n=== Code Style and Best Practices ===")
for evaluation in style_results:
    print(f"\\n{evaluation['language'].title()}: {evaluation['prompt']}")
    print(f"  Style Score: {evaluation['style_score']:.3f}")
    
    if evaluation['best_practices']:
        print("  Best Practices Followed:")
        for practice in evaluation['best_practices']:
            print(f"    {practice}")
    
    if evaluation['issues']:
        print("  Style Issues:")
        for issue in evaluation['issues']:
            print(f"    • {issue}")

# 6. Overall Code Quality Report
def generate_code_quality_report(basic_results, security_results, performance_results, style_results):
    """Generate comprehensive code quality report"""
    
    overall_scores = []
    
    for i in range(len(prompts)):
        # Weighted overall score
        weights = {
            "correctness": 0.3,
            "security": 0.25,
            "performance": 0.25,
            "style": 0.2
        }
        
        correctness_score = basic_results.get("bleu", 0.8)  # Use BLEU as correctness proxy
        security_score = security_results[i]["security_score"]
        performance_score = performance_results[i]["performance_score"]
        style_score = style_results[i]["style_score"]
        
        overall_score = (
            correctness_score * weights["correctness"] +
            security_score * weights["security"] +
            performance_score * weights["performance"] +
            style_score * weights["style"]
        )
        
        quality_level = "Excellent" if overall_score >= 0.9 else "Good" if overall_score >= 0.75 else "Needs Improvement"
        
        overall_scores.append({
            "prompt": prompts[i],
            "language": languages[i],
            "correctness": correctness_score,
            "security": security_score,
            "performance": performance_score,
            "style": style_score,
            "overall_score": overall_score,
            "quality_level": quality_level
        })
    
    return overall_scores

quality_report = generate_code_quality_report(results, security_assessment, performance_results, style_results)

print("\\n=== Overall Code Quality Report ===")
for report in quality_report:
    print(f"\\n{report['language'].title()}: {report['prompt'][:60]}...")
    print(f"  Correctness: {report['correctness']:.3f}")
    print(f"  Security: {report['security']:.3f}")
    print(f"  Performance: {report['performance']:.3f}")
    print(f"  Style: {report['style']:.3f}")
    print(f"  Overall Score: {report['overall_score']:.3f} ({report['quality_level']})")

print("\\n=== Code Generation Evaluation Complete ===")
print("Comprehensive evaluation of code correctness, security, performance, and style across multiple languages.")`}</code>
                </pre>
              </div>

              <div>
                <h4 className="font-semibold mb-2">Real-World Code Generation Workflows</h4>
                <pre className="text-sm overflow-x-auto bg-muted p-4 rounded-lg">
                  <code>{`# Real-world code generation evaluation workflow
from blazemetrics import CodeEvaluator, BlazeMetricsClient
import subprocess
import tempfile
import os

class CodeGenerationWorkflow:
    def __init__(self):
        self.code_evaluator = CodeEvaluator(
            languages=["python", "javascript", "java", "sql"],
            security_checks=True,
            performance_analysis=True
        )
        self.client = BlazeMetricsClient(enable_analytics=True)
    
    def evaluate_code_generation_model(self, model_name, test_cases):
        """Evaluate a code generation model on multiple test cases"""
        
        results = {
            "model_name": model_name,
            "test_results": [],
            "overall_metrics": {},
            "language_breakdown": {}
        }
        
        all_generated = []
        all_references = []
        all_prompts = []
        
        for test_case in test_cases:
            # Generate code (in practice, this would call your model)
            generated_code = self.simulate_code_generation(test_case["prompt"], test_case["language"])
            
            # Evaluate individual test case
            case_result = self.evaluate_single_case(
                test_case["prompt"],
                generated_code,
                test_case["reference"],
                test_case["language"]
            )
            
            results["test_results"].append(case_result)
            all_generated.append(generated_code)
            all_references.append(test_case["reference"])
            all_prompts.append(test_case["prompt"])
        
        # Overall evaluation
        overall_results = self.code_evaluator.evaluate(all_prompts, all_generated, all_references)
        results["overall_metrics"] = overall_results
        
        # Language-specific breakdown
        languages = set(tc["language"] for tc in test_cases)
        for lang in languages:
            lang_cases = [tc for tc in test_cases if tc["language"] == lang]
            lang_generated = [self.simulate_code_generation(tc["prompt"], tc["language"]) for tc in lang_cases]
            lang_references = [tc["reference"] for tc in lang_cases]
            lang_prompts = [tc["prompt"] for tc in lang_cases]
            
            lang_evaluator = CodeEvaluator(languages=[lang])
            lang_results = lang_evaluator.evaluate(lang_prompts, lang_generated, lang_references)
            results["language_breakdown"][lang] = lang_results
        
        return results
    
    def simulate_code_generation(self, prompt, language):
        """Simulate code generation (replace with actual model calls)"""
        
        # This would be replaced with actual model inference
        templates = {
            "python": {
                "function": "def {name}({params}):\\n    # TODO: Implement\\n    pass",
                "class": "class {name}:\\n    def __init__(self):\\n        pass"
            },
            "javascript": {
                "function": "function {name}({params}) {{\\n    // TODO: Implement\\n}}",
                "async": "async function {name}({params}) {{\\n    // TODO: Implement\\n}}"
            }
        }
        
        # Simple template-based generation for demo
        if "function" in prompt.lower() and language in templates:
            return templates[language]["function"].format(name="generatedFunction", params="param1, param2")
        
        return f"// Generated {language} code for: {prompt[:30]}..."
    
    def evaluate_single_case(self, prompt, generated_code, reference_code, language):
        """Evaluate a single code generation case"""
        
        # Basic metrics
        basic_metrics = self.client.compute_metrics([generated_code], [[reference_code]])
        
        # Functional correctness (simplified)
        functional_score = self.test_functional_correctness(generated_code, language)
        
        # Security analysis
        security_score = self.analyze_security(generated_code, language)
        
        # Performance estimation
        performance_score = self.estimate_performance(generated_code, language)
        
        return {
            "prompt": prompt,
            "language": language,
            "basic_metrics": {k: v[0] for k, v in basic_metrics.items()},
            "functional_correctness": functional_score,
            "security_score": security_score,
            "performance_score": performance_score,
            "overall_quality": (
                sum(basic_metrics.values(), [0])[0] * 0.3 +
                functional_score * 0.3 +
                security_score * 0.2 +
                performance_score * 0.2
            )
        }
    
    def test_functional_correctness(self, code, language):
        """Test functional correctness of generated code"""
        
        try:
            if language == "python":
                # Create temporary file and test execution
                with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
                    f.write(code)
                    f.flush()
                    
                    # Try to compile (syntax check)
                    result = subprocess.run(['python', '-m', 'py_compile', f.name], 
                                          capture_output=True, text=True)
                    
                    os.unlink(f.name)
                    return 1.0 if result.returncode == 0 else 0.5
            
            elif language == "javascript":
                # Basic syntax validation for JavaScript
                if code.count('{') == code.count('}') and code.count('(') == code.count(')'):
                    return 0.8  # Basic syntax seems correct
                else:
                    return 0.3  # Syntax issues
            
            return 0.7  # Default score for other languages
            
        except Exception:
            return 0.0
    
    def analyze_security(self, code, language):
        """Analyze security aspects of generated code"""
        
        security_score = 1.0
        
        # Common security anti-patterns
        dangerous_patterns = {
            "python": ["eval(", "exec(", "os.system(", "__import__"],
            "javascript": ["eval(", "innerHTML", "document.write(", "setTimeout("],
            "sql": ["'; DROP", "UNION SELECT", "-- ", "/*"],
            "java": ["Runtime.exec(", "ProcessBuilder(", "Class.forName("]
        }
        
        if language in dangerous_patterns:
            for pattern in dangerous_patterns[language]:
                if pattern in code:
                    security_score -= 0.2
        
        # Check for input validation
        if language in ["python", "javascript"] and "input" in code.lower():
            if "validate" not in code.lower() and "sanitize" not in code.lower():
                security_score -= 0.1
        
        return max(0.0, security_score)
    
    def estimate_performance(self, code, language):
        """Estimate performance characteristics of generated code"""
        
        performance_score = 0.8  # Default score
        
        # Check for common performance issues
        if "for" in code and "for" in code:  # Nested loops
            nested_loops = code.count("for") > 1 and "    for" in code
            if nested_loops:
                performance_score -= 0.2
        
        # Check for efficient patterns
        if language == "python":
            if "list comprehension" in code or "[" in code and "for" in code and "]" in code:
                performance_score += 0.1
        
        elif language == "javascript":
            if "map(" in code or "filter(" in code or "reduce(" in code:
                performance_score += 0.1
        
        return min(1.0, performance_score)

# Example usage of the workflow
workflow = CodeGenerationWorkflow()

# Define test cases
test_cases = [
    {
        "prompt": "Write a function to sort an array of integers",
        "language": "python",
        "reference": "def sort_array(arr):\\n    return sorted(arr)"
    },
    {
        "prompt": "Create an async function to fetch user data",
        "language": "javascript", 
        "reference": "async function fetchUserData(userId) {\\n    const response = await fetch('/api/users/' + userId);\\n    return response.json();\\n}"
    },
    {
        "prompt": "Implement a binary tree node class",
        "language": "java",
        "reference": "public class TreeNode {\\n    int val;\\n    TreeNode left;\\n    TreeNode right;\\n    TreeNode(int val) { this.val = val; }\\n}"
    }
]

print("=== Real-World Code Generation Workflow ===")

# Evaluate the model
evaluation_results = workflow.evaluate_code_generation_model("TestModel-v1", test_cases)

print(f"\\nModel: {evaluation_results['model_name']}")
print("\\nOverall Metrics:")
for metric, value in evaluation_results["overall_metrics"].items():
    print(f"  {metric}: {value:.3f}")

print("\\nLanguage Breakdown:")
for language, metrics in evaluation_results["language_breakdown"].items():
    print(f"  {language.title()}:")
    for metric, value in metrics.items():
        print(f"    {metric}: {value:.3f}")

print("\\nIndividual Test Results:")
for i, result in enumerate(evaluation_results["test_results"]):
    print(f"\\nTest {i+1}: {result['prompt'][:50]}...")
    print(f"  Language: {result['language']}")
    print(f"  Functional Correctness: {result['functional_correctness']:.3f}")
    print(f"  Security Score: {result['security_score']:.3f}")
    print(f"  Performance Score: {result['performance_score']:.3f}")
    print(f"  Overall Quality: {result['overall_quality']:.3f}")

# Model comparison workflow
def compare_code_generation_models(models, test_cases):
    """Compare multiple code generation models"""
    
    comparison_results = {}
    
    for model_name in models:
        print(f"\\nEvaluating {model_name}...")
        results = workflow.evaluate_code_generation_model(model_name, test_cases)
        comparison_results[model_name] = results
    
    # Generate comparison report
    print("\\n=== Model Comparison Report ===")
    
    metrics_to_compare = ["bleu", "rouge1", "meteor"]
    
    for metric in metrics_to_compare:
        print(f"\\n{metric.upper()} Scores:")
        for model_name, results in comparison_results.items():
            score = results["overall_metrics"].get(metric, 0.0)
            print(f"  {model_name}: {score:.3f}")
    
    # Find best model per language
    print("\\nBest Model Per Language:")
    all_languages = set()
    for results in comparison_results.values():
        all_languages.update(results["language_breakdown"].keys())
    
    for language in all_languages:
        best_model = None
        best_score = 0.0
        
        for model_name, results in comparison_results.items():
            if language in results["language_breakdown"]:
                lang_score = results["language_breakdown"][language].get("bleu", 0.0)
                if lang_score > best_score:
                    best_score = lang_score
                    best_model = model_name
        
        print(f"  {language.title()}: {best_model} ({best_score:.3f})")
    
    return comparison_results

# Compare multiple models
models_to_compare = ["GPT-4-Code", "CodeLlama-7B", "StarCoder-15B"]
comparison_results = compare_code_generation_models(models_to_compare, test_cases)

print("\\n=== Code Generation Workflow Complete ===")
print("Comprehensive evaluation and comparison of code generation models across multiple languages and quality dimensions.")`}</code>
                </pre>
              </div>
            </CardContent>
          </Card>
        </TabsContent>

        <TabsContent value="flagship" className="space-y-6">
          <Card>
            <CardHeader>
              <CardTitle className="flex items-center gap-2">
                <Target className="w-5 h-5 text-indigo-500" />
                Flagship Content Examples
              </CardTitle>
              <CardDescription>
                Showcase demonstrations with head-to-head comparisons and real-world impact
              </CardDescription>
            </CardHeader>
            <CardContent className="space-y-4">
              <Alert>
                <Zap className="h-4 w-4" />
                <AlertDescription>
                  Based on real backend examples from <code>use_cases/</code> directory with complete working implementations
                </AlertDescription>
              </Alert>

              <div>
                <h4 className="font-semibold mb-2">"ChatGPT vs Claude vs Llama" Head-to-Head Comparison</h4>
                <pre className="text-sm overflow-x-auto bg-muted p-4 rounded-lg">
                  <code>{`# Head-to-head LLM comparison with real metrics
from blazemetrics import BlazeMetricsClient
from blazemetrics.llm_judge import LLMJudge
import openai
import anthropic
from transformers import pipeline

# Setup LLM providers
openai_client = openai.OpenAI(api_key="your-openai-key")
anthropic_client = anthropic.Anthropic(api_key="your-anthropic-key")
llama_pipeline = pipeline("text-generation", model="meta-llama/Llama-2-7b-chat-hf")

# Test prompts for comparison
test_prompts = [
    "Explain quantum computing in simple terms for a high school student.",
    "Write a professional email declining a job offer politely.",
    "Create a Python function to find the longest palindrome in a string.",
    "Summarize the key causes and effects of climate change.",
    "Write a creative short story opening about a time traveler."
]

# Reference answers (human-written gold standard)
reference_answers = [
    "Quantum computing uses quantum mechanics principles like superposition and entanglement to process information differently than classical computers, potentially solving certain problems exponentially faster.",
    "Dear [Hiring Manager], Thank you for the generous job offer. After careful consideration, I have decided to pursue another opportunity that better aligns with my career goals. I appreciate your time and wish you success in finding the right candidate.",
    "def longest_palindrome(s):\\n    def expand_around_center(left, right):\\n        while left >= 0 and right < len(s) and s[left] == s[right]:\\n            left -= 1\\n            right += 1\\n        return s[left+1:right]\\n    \\n    longest = ''\\n    for i in range(len(s)):\\n        # Odd length palindromes\\n        palindrome1 = expand_around_center(i, i)\\n        # Even length palindromes\\n        palindrome2 = expand_around_center(i, i+1)\\n        \\n        for p in [palindrome1, palindrome2]:\\n            if len(p) > len(longest):\\n                longest = p\\n    \\n    return longest",
    "Climate change is primarily caused by greenhouse gas emissions from human activities, leading to global temperature rise, extreme weather events, sea level rise, and ecosystem disruption.",
    "The brass pocket watch felt warm against Maya's palm as the familiar blue light began to swirl around her feet. She had exactly thirty seconds before the temporal displacement would activate, and this time, she wasn't sure she wanted to go back."
]

def get_chatgpt_response(prompt):
    """Get response from ChatGPT"""
    response = openai_client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=300,
        temperature=0.7
    )
    return response.choices[0].message.content

def get_claude_response(prompt):
    """Get response from Claude"""
    response = anthropic_client.messages.create(
        model="claude-3-sonnet-20240229",
        max_tokens=300,
        messages=[{"role": "user", "content": prompt}]
    )
    return response.content[0].text

def get_llama_response(prompt):
    """Get response from Llama"""
    response = llama_pipeline(prompt, max_new_tokens=200, temperature=0.7)
    return response[0]['generated_text'][len(prompt):].strip()

# Setup evaluation
client = BlazeMetricsClient(
    enable_analytics=True,
    metrics_include=["bleu", "rouge1", "rouge2", "meteor", "chrf"]
)

# Setup LLM judge for quality assessment
judge = LLMJudge(
    provider="openai",
    api_key="your-openai-key",
    model="gpt-4o",
    system_prompt="""Rate responses on:
    - Accuracy (0-1): Factual correctness
    - Clarity (0-1): Clear communication
    - Completeness (0-1): Fully addresses the prompt
    - Creativity (0-1): Original and engaging (when appropriate)
    - Helpfulness (0-1): Practical value to the user
    Return JSON with scores and overall_quality (0-1)."""
)

print("=== ChatGPT vs Claude vs Llama: Head-to-Head Comparison ===")

# Collect responses from all models
all_responses = {"ChatGPT": [], "Claude": [], "Llama": []}
model_functions = {
    "ChatGPT": get_chatgpt_response,
    "Claude": get_claude_response, 
    "Llama": get_llama_response
}

for prompt in test_prompts:
    print(f"\\nPrompt: {prompt}")
    for model_name, model_func in model_functions.items():
        try:
            response = model_func(prompt)
            all_responses[model_name].append(response)
            print(f"  {model_name}: {response[:100]}...")
        except Exception as e:
            print(f"  {model_name}: Error - {str(e)}")
            all_responses[model_name].append("Error generating response")

# Evaluate each model
model_results = {}
for model_name, responses in all_responses.items():
    print(f"\\n=== {model_name} Evaluation ===")
    
    # Basic metrics
    metrics = client.compute_metrics(responses, reference_answers)
    aggregated = client.aggregate_metrics(metrics)
    
    print("Basic Metrics:")
    for metric, value in aggregated.items():
        print(f"  {metric}: {value:.3f}")
    
    # Advanced quality assessment
    client.set_factuality_scorer(judge.score)
    quality_results = client.evaluate_factuality(responses, reference_answers)
    
    # Calculate average quality scores
    avg_quality = {}
    quality_dimensions = ["accuracy", "clarity", "completeness", "creativity", "helpfulness", "overall_quality"]
    
    for dimension in quality_dimensions:
        scores = [result.get(dimension, 0.0) for result in quality_results]
        avg_quality[dimension] = sum(scores) / len(scores)
    
    print("\\nQuality Assessment:")
    for dimension, score in avg_quality.items():
        print(f"  {dimension.title()}: {score:.3f}")
    
    model_results[model_name] = {
        "basic_metrics": aggregated,
        "quality_scores": avg_quality,
        "responses": responses
    }

# Head-to-head comparison
print("\\n=== HEAD-TO-HEAD COMPARISON RESULTS ===")

# Overall winner determination
def calculate_overall_score(basic_metrics, quality_scores):
    """Calculate weighted overall score"""
    weights = {
        "basic_metrics": 0.4,  # BLEU, ROUGE, etc.
        "quality_scores": 0.6  # LLM judge scores
    }
    
    basic_avg = sum(basic_metrics.values()) / len(basic_metrics)
    quality_avg = quality_scores.get("overall_quality", 0.0)
    
    return (basic_avg * weights["basic_metrics"] + 
            quality_avg * weights["quality_scores"])

overall_scores = {}
for model_name, results in model_results.items():
    overall_scores[model_name] = calculate_overall_score(
        results["basic_metrics"], 
        results["quality_scores"]
    )

# Rank models
ranked_models = sorted(overall_scores.items(), key=lambda x: x[1], reverse=True)

print("\\nOverall Rankings:")
for i, (model_name, score) in enumerate(ranked_models):
    medal = "🥇" if i == 0 else "🥈" if i == 1 else "🥉" if i == 2 else f"{i+1}."
    print(f"  {medal} {model_name}: {score:.3f}")

# Category winners
categories = {
    "Most Accurate": "accuracy",
    "Clearest Communication": "clarity", 
    "Most Complete": "completeness",
    "Most Creative": "creativity",
    "Most Helpful": "helpfulness"
}

print("\\nCategory Winners:")
for category, dimension in categories.items():
    best_model = max(model_results.items(), 
                    key=lambda x: x[1]["quality_scores"].get(dimension, 0.0))
    score = best_model[1]["quality_scores"].get(dimension, 0.0)
    print(f"  {category}: {best_model[0]} ({score:.3f})")

# Performance analysis by prompt type
prompt_types = [
    "Technical Explanation",
    "Professional Writing", 
    "Code Generation",
    "Factual Summary",
    "Creative Writing"
]

print("\\nPerformance by Prompt Type:")
for i, prompt_type in enumerate(prompt_types):
    print(f"\\n{prompt_type}:")
    for model_name, results in model_results.items():
        # Get quality score for this specific prompt
        quality_results = client.evaluate_factuality([results["responses"][i]], [reference_answers[i]])
        prompt_score = quality_results[0].get("overall_quality", 0.0)
        print(f"  {model_name}: {prompt_score:.3f}")

print("\\n=== Key Insights ===")
winner = ranked_models[0][0]
runner_up = ranked_models[1][0]
print(f"• {winner} leads overall with superior {max(model_results[winner]['quality_scores'].items(), key=lambda x: x[1])[0]}")
print(f"• {runner_up} excels in {max(model_results[runner_up]['quality_scores'].items(), key=lambda x: x[1])[0]}")
print(f"• Largest performance gap in {max(categories.items(), key=lambda x: max(model_results[m]['quality_scores'].get(x[1], 0) for m in model_results) - min(model_results[m]['quality_scores'].get(x[1], 0) for m in model_results))[0]}")

print("\\n=== ChatGPT vs Claude vs Llama Comparison Complete ===")
print("Comprehensive head-to-head evaluation with real metrics and detailed analysis.")`}</code>
                </pre>
              </div>

              <div>
                <h4 className="font-semibold mb-2">"From Toxic to Safe" Content Moderation Pipeline</h4>
                <pre className="text-sm overflow-x-auto bg-muted p-4 rounded-lg">
                  <code>{`# Complete content moderation pipeline demonstration
from blazemetrics import BlazeMetricsClient
from blazemetrics.llm_guardrails import RealTimeLLMGuardrail
import time

# Toxic content samples (for demonstration - real content would be user-generated)
toxic_content_samples = [
    "You're such an idiot, I hate people like you!",
    "This product is garbage and the company should burn down.",
    "All people from that country are criminals and terrorists.",
    "I'm going to find where you live and make you pay.",
    "Women are too stupid to understand technology.",
    "That politician should be assassinated for their policies.",
    "Your religion is a cult and you're all brainwashed morons.",
    "I hope you get cancer and die slowly.",
]

# Expected safe versions (what the moderation should produce)
safe_versions = [
    "I disagree with your perspective on this topic.",
    "I'm not satisfied with this product and would like a refund.",
    "I have concerns about immigration policies in that region.",
    "I'm very frustrated with this situation and need resolution.",
    "I think there may be a gender gap in technology education.",
    "I strongly disagree with that politician's policy positions.",
    "I have different religious beliefs and perspectives.",
    "I'm very upset about this situation.",
]

# Setup comprehensive content moderation
client = BlazeMetricsClient(
    blocklist=[
        "idiot", "hate", "garbage", "burn down", "criminals", "terrorists",
        "assassinated", "cancer", "die", "stupid", "morons", "brainwashed"
    ],
    regexes=[
        r"\\b(kill|murder|assassinate|die)\\b",
        r"\\b(hate|stupid|idiot|moron)\\b",
        r"\\b(terrorist|criminal)s?\\b"
    ],
    redact_pii=True,
    enable_analytics=True
)

# Advanced guardrail system
def toxicity_classifier(text):
    """Simulate toxicity classification (replace with real model)"""
    toxic_indicators = [
        "hate", "kill", "die", "stupid", "idiot", "garbage", 
        "terrorist", "assassinate", "cancer", "burn"
    ]
    
    toxicity_score = 0.0
    for indicator in toxic_indicators:
        if indicator in text.lower():
            toxicity_score += 0.15
    
    toxicity_score = min(1.0, toxicity_score)
    
    return {
        "label": "toxic" if toxicity_score > 0.5 else "safe",
        "confidence": toxicity_score if toxicity_score > 0.5 else 1.0 - toxicity_score,
        "toxicity_score": toxicity_score
    }

guardrail = RealTimeLLMGuardrail(
    model=toxicity_classifier,
    labels=["safe", "toxic"],
    enforcement={
        "toxic": "rewrite"  # Automatically rewrite toxic content
    },
    on_violation=lambda event: print(f"[MODERATION] {event}")
)

print("=== 'From Toxic to Safe' Content Moderation Pipeline ===")

# Process each toxic sample through the pipeline
moderation_results = []

for i, toxic_text in enumerate(toxic_content_samples):
    print(f"\\n--- Sample {i+1} ---")
    print(f"Original: {toxic_text}")
    
    # Step 1: Basic safety check
    safety_result = client.check_safety([toxic_text])[0]
    
    # Step 2: Advanced guardrail processing
    guardrail_result = guardrail.validate_full(toxic_text)
    
    # Step 3: Generate safe alternative (simulated)
    safe_alternative = safe_versions[i]  # In practice, use LLM rewriting
    
    # Step 4: Verify safety of rewritten content
    safe_check = client.check_safety([safe_alternative])[0]
    
    result = {
        "original": toxic_text,
        "safety_score": safety_result.get("safety_score", 0.0),
        "blocked": safety_result.get("blocked", False),
        "toxicity_detected": guardrail_result.get("label") == "toxic",
        "toxicity_confidence": guardrail_result.get("confidence", 0.0),
        "safe_alternative": safe_alternative,
        "safe_score": safe_check.get("safety_score", 1.0),
        "transformation_success": safe_check.get("safety_score", 1.0) > 0.8
    }
    
    moderation_results.append(result)
    
    print(f"Safety Score: {result['safety_score']:.3f}")
    print(f"Blocked: {result['blocked']}")
    print(f"Toxicity: {result['toxicity_confidence']:.3f}")
    print(f"Safe Version: {result['safe_alternative']}")
    print(f"Transformation Success: {'✓' if result['transformation_success'] else '✗'}")

# Pipeline effectiveness analysis
print("\\n=== Pipeline Effectiveness Analysis ===")

total_samples = len(moderation_results)
detected_toxic = sum(1 for r in moderation_results if r["toxicity_detected"])
successfully_transformed = sum(1 for r in moderation_results if r["transformation_success"])
avg_safety_improvement = sum(r["safe_score"] - r["safety_score"] for r in moderation_results) / total_samples

print(f"Detection Rate: {detected_toxic}/{total_samples} ({detected_toxic/total_samples:.1%})")
print(f"Transformation Success: {successfully_transformed}/{total_samples} ({successfully_transformed/total_samples:.1%})")
print(f"Average Safety Improvement: {avg_safety_improvement:.3f}")

# Real-time moderation simulation
print("\\n=== Real-Time Moderation Simulation ===")

def simulate_user_content_stream():
    """Simulate real-time user content stream"""
    content_stream = [
        "This is a normal comment about the weather.",
        "I hate this stupid website and everyone on it!",
        "Great article, thanks for sharing!",
        "You people are all idiots and should die.",
        "Can someone help me with this technical issue?",
        "This company is garbage and should burn down!",
        "I love this community, very helpful people.",
        "All politicians are corrupt criminals who deserve death."
    ]
    
    for content in content_stream:
        yield content
        time.sleep(0.5)  # Simulate real-time delay

moderation_stats = {
    "total_processed": 0,
    "toxic_detected": 0,
    "safe_passed": 0,
    "rewritten": 0,
    "blocked": 0
}

print("Processing real-time content stream...")
for content in simulate_user_content_stream():
    moderation_stats["total_processed"] += 1
    
    # Process through moderation pipeline
    safety_result = client.check_safety([content])[0]
    guardrail_result = guardrail.validate_full(content)
    
    if guardrail_result.get("label") == "toxic":
        moderation_stats["toxic_detected"] += 1
        
        if guardrail_result.get("enforcement") == "rewrite":
            moderation_stats["rewritten"] += 1
            print(f"[REWRITTEN] {content[:50]}... → Safe version generated")
        else:
            moderation_stats["blocked"] += 1
            print(f"[BLOCKED] {content[:50]}...")
    else:
        moderation_stats["safe_passed"] += 1
        print(f"[PASSED] {content[:50]}...")

print("\\n=== Real-Time Moderation Statistics ===")
for stat, value in moderation_stats.items():
    if stat == "total_processed":
        print(f"{stat.replace('_', ' ').title()}: {value}")
    else:
        percentage = (value / moderation_stats["total_processed"]) * 100
        print(f"{stat.replace('_', ' ').title()}: {value} ({percentage:.1f}%)")

# Content moderation model card
def generate_moderation_model_card(results, stats):
    """Generate model card for content moderation system"""
    
    model_card = f"""
# Content Moderation System Model Card

## Model Overview
- **Purpose**: Real-time content moderation and toxicity detection
- **Approach**: Multi-layered pipeline with blocklists, regex patterns, and ML classification
- **Deployment**: Real-time streaming content processing

## Performance Metrics
- **Detection Accuracy**: {(stats['toxic_detected'] / (stats['toxic_detected'] + stats['safe_passed'])) * 100:.1f}%
- **False Positive Rate**: {((stats['total_processed'] - stats['toxic_detected'] - stats['safe_passed']) / stats['total_processed']) * 100:.1f}%
- **Processing Speed**: {stats['total_processed']} messages/second (simulated)
- **Transformation Success**: {(successfully_transformed / total_samples) * 100:.1f}%

## Safety Measures
- **Blocklist Coverage**: {len(client.config.blocklist)} terms
- **Regex Patterns**: {len(client.config.regexes)} patterns
- **PII Protection**: Enabled
- **Real-time Monitoring**: Enabled

## Limitations
- Context-dependent toxicity may be missed
- Cultural and linguistic biases possible
- Requires regular updates to blocklists
- May over-moderate edge cases

## Ethical Considerations
- Balances free speech with safety
- Transparent moderation decisions
- Appeals process recommended
- Regular bias auditing performed
"""
    
    return model_card

model_card = generate_moderation_model_card(moderation_results, moderation_stats)
print("\\n=== Content Moderation Model Card ===")
print(model_card)

print("\\n=== 'From Toxic to Safe' Pipeline Complete ===")
print("Comprehensive content moderation with real-time processing and safety transformation.")`}</code>
                </pre>
              </div>

              <div>
                <h4 className="font-semibold mb-2">"Medical AI Accuracy Check" with HIPAA Compliance</h4>
                <pre className="text-sm overflow-x-auto bg-muted p-4 rounded-lg">
                  <code>{`# Medical AI evaluation with HIPAA compliance demonstration
from blazemetrics import BlazeMetricsClient
from blazemetrics.safety_evaluator import SafetyEvaluator
import hashlib
import re

# Medical AI diagnostic outputs (anonymized examples)
medical_diagnoses = [
    "Patient presents with acute chest pain, elevated troponin levels, and ECG changes consistent with myocardial infarction. Recommend immediate cardiac catheterization.",
    "Symptoms and imaging suggest pneumonia. Patient John Smith (DOB: 1985-03-15, SSN: 123-45-6789) should start antibiotic therapy immediately.",  # Contains PII
    "Chronic fatigue and joint pain may indicate autoimmune condition. Further testing recommended including ANA, RF, and anti-CCP antibodies.",
    "Skin lesion shows irregular borders and color variation. Biopsy recommended to rule out melanoma. Patient contact: jane.doe@email.com",  # Contains PII
    "Blood glucose levels consistently above 200 mg/dL with symptoms of polyuria and polydipsia indicate Type 2 diabetes mellitus.",
    "Cognitive assessment shows memory impairment and confusion. Consider Alzheimer's disease evaluation with neuroimaging and CSF biomarkers.",
]

# Gold standard diagnoses from medical professionals
gold_standard = [
    "Acute ST-elevation myocardial infarction based on clinical presentation, cardiac biomarkers, and electrocardiographic findings. Emergency percutaneous coronary intervention indicated.",
    "Community-acquired pneumonia confirmed by chest X-ray and clinical symptoms. Initiate empirical antibiotic treatment per guidelines.",
    "Clinical presentation suggestive of rheumatoid arthritis. Recommend comprehensive autoimmune panel and rheumatology consultation.",
    "Suspicious pigmented lesion with ABCDE criteria positive. Urgent dermatology referral for biopsy and histopathological examination.",
    "New-onset Type 2 diabetes mellitus based on hyperglycemia and classic symptoms. Initiate metformin and diabetes education.",
    "Mild cognitive impairment with possible early-stage dementia. Comprehensive neuropsychological testing and brain MRI recommended.",
]

# Patient demographics for bias analysis (anonymized)
patient_demographics = [
    {"age_group": "55-65", "gender": "M", "ethnicity": "Caucasian", "insurance": "private"},
    {"age_group": "35-45", "gender": "M", "ethnicity": "Hispanic", "insurance": "medicaid"},
    {"age_group": "45-55", "gender": "F", "ethnicity": "African American", "insurance": "private"},
    {"age_group": "65-75", "gender": "F", "ethnicity": "Caucasian", "insurance": "medicare"},
    {"age_group": "25-35", "gender": "M", "ethnicity": "Asian", "insurance": "private"},
    {"age_group": "75-85", "gender": "F", "ethnicity": "Caucasian", "insurance": "medicare"},
]

# Setup HIPAA-compliant evaluation
client = BlazeMetricsClient(
    redact_pii=True,
    blocklist=["SSN", "social security", "patient name", "DOB", "date of birth"],
    regexes=[
        r"\\b\\d{3}-\\d{2}-\\d{4}\\b",  # SSN pattern
        r"\\b\\d{4}-\\d{2}-\\d{2}\\b",  # Date pattern
        r"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b",  # Email pattern
        r"\\b[A-Z][a-z]+ [A-Z][a-z]+\\b",  # Name pattern (simplified)
    ],
    enable_analytics=True
)

safety_evaluator = SafetyEvaluator(
    bias_detection=True,
    fairness_metrics=True,
    demographic_parity=True
)

print("=== Medical AI Accuracy Check with HIPAA Compliance ===")

# 1. HIPAA Compliance Check
print("\\n=== HIPAA Compliance Analysis ===")

pii_violations = []
for i, diagnosis in enumerate(medical_diagnoses):
    safety_result = client.check_safety([diagnosis])[0]
    
    pii_detected = safety_result.get("pii_detected", False)
    redacted_text = safety_result.get("redacted", diagnosis)
    
    if pii_detected:
        pii_violations.append({
            "case_id": i + 1,
            "original": diagnosis,
            "redacted": redacted_text,
            "pii_types": safety_result.get("pii_types", [])
        })
    
    print(f"\\nCase {i+1}:")
    print(f"  PII Detected: {'✗ YES' if pii_detected else '✓ NO'}")
    if pii_detected:
        print(f"  Original: {diagnosis[:80]}...")
        print(f"  Redacted: {redacted_text[:80]}...")

print(f"\\nHIPAA Compliance Summary:")
print(f"  Total Cases: {len(medical_diagnoses)}")
print(f"  PII Violations: {len(pii_violations)}")
print(f"  Compliance Rate: {((len(medical_diagnoses) - len(pii_violations)) / len(medical_diagnoses)) * 100:.1f}%")

# 2. Diagnostic Accuracy Evaluation
print("\\n=== Diagnostic Accuracy Evaluation ===")

# Use redacted versions for accuracy evaluation
redacted_diagnoses = []
for diagnosis in medical_diagnoses:
    safety_result = client.check_safety([diagnosis])[0]
    redacted_diagnoses.append(safety_result.get("redacted", diagnosis))

# Compute diagnostic accuracy metrics
accuracy_metrics = client.compute_metrics(redacted_diagnoses, gold_standard)
aggregated_accuracy = client.aggregate_metrics(accuracy_metrics)

print("Diagnostic Accuracy Metrics:")
for metric, value in aggregated_accuracy.items():
    print(f"  {metric}: {value:.3f}")

# Medical-specific accuracy analysis
def analyze_medical_accuracy(ai_diagnoses, gold_diagnoses):
    """Analyze medical diagnostic accuracy with clinical relevance"""
    
    clinical_accuracy = {
        "correct_primary_diagnosis": 0,
        "correct_treatment_recommendation": 0,
        "missed_critical_findings": 0,
        "false_positive_urgency": 0
    }
    
    # Keywords for critical conditions
    critical_keywords = ["myocardial infarction", "pneumonia", "melanoma", "diabetes", "dementia"]
    urgent_keywords = ["immediate", "emergency", "urgent", "stat"]
    
    for ai_diag, gold_diag in zip(ai_diagnoses, gold_diagnoses):
        # Check primary diagnosis accuracy
        ai_conditions = [kw for kw in critical_keywords if kw in ai_diag.lower()]
        gold_conditions = [kw for kw in critical_keywords if kw in gold_diag.lower()]
        
        if ai_conditions and gold_conditions and ai_conditions[0] == gold_conditions[0]:
            clinical_accuracy["correct_primary_diagnosis"] += 1
        
        # Check treatment recommendations
        ai_urgent = any(kw in ai_diag.lower() for kw in urgent_keywords)
        gold_urgent = any(kw in gold_diag.lower() for kw in urgent_keywords)
        
        if ai_urgent == gold_urgent:
            clinical_accuracy["correct_treatment_recommendation"] += 1
        
        # Check for missed critical findings
        if gold_urgent and not ai_urgent:
            clinical_accuracy["missed_critical_findings"] += 1
        
        # Check for false positive urgency
        if ai_urgent and not gold_urgent:
            clinical_accuracy["false_positive_urgency"] += 1
    
    # Convert to percentages
    total_cases = len(ai_diagnoses)
    for key in clinical_accuracy:
        clinical_accuracy[key] = (clinical_accuracy[key] / total_cases) * 100
    
    return clinical_accuracy

clinical_results = analyze_medical_accuracy(redacted_diagnoses, gold_standard)

print("\\nClinical Accuracy Analysis:")
for metric, percentage in clinical_results.items():
    status = "✓" if percentage >= 80 else "⚠️" if percentage >= 60 else "✗"
    print(f"  {metric.replace('_', ' ').title()}: {percentage:.1f}% {status}")

# 3. Bias Detection in Medical AI
print("\\n=== Medical AI Bias Detection ===")

def analyze_medical_bias(diagnoses, demographics):
    """Analyze bias in medical AI across demographic groups"""
    
    bias_analysis = {}
    
    # Group by demographic attributes
    for attribute in ["age_group", "gender", "ethnicity", "insurance"]:
        groups = {}
        
        for i, demo in enumerate(demographics):
            group = demo[attribute]
            if group not in groups:
                groups[group] = {"diagnoses": [], "accuracy_scores": []}
            
            groups[group]["diagnoses"].append(diagnoses[i])
            
            # Calculate accuracy score for this diagnosis (simplified)
            ai_diag = diagnoses[i].lower()
            gold_diag = gold_standard[i].lower()
            
            # Simple word overlap accuracy
            ai_words = set(ai_diag.split())
            gold_words = set(gold_diag.split())
            overlap = len(ai_words.intersection(gold_words))
            accuracy = overlap / len(gold_words.union(ai_words)) if gold_words.union(ai_words) else 0
            
            groups[group]["accuracy_scores"].append(accuracy)
        
        # Calculate bias metrics
        group_accuracies = {}
        for group, data in groups.items():
            avg_accuracy = sum(data["accuracy_scores"]) / len(data["accuracy_scores"])
            group_accuracies[group] = avg_accuracy
        
        # Calculate bias variance
        accuracies = list(group_accuracies.values())
        bias_variance = max(accuracies) - min(accuracies) if accuracies else 0
        
        bias_analysis[attribute] = {
            "group_accuracies": group_accuracies,
            "bias_variance": bias_variance,
            "bias_detected": bias_variance > 0.1  # 10% threshold
        }
    
    return bias_analysis

bias_results = analyze_medical_bias(redacted_diagnoses, patient_demographics)

print("Medical AI Bias Analysis:")
for attribute, analysis in bias_results.items():
    print(f"\\n{attribute.replace('_', ' ').title()}:")
    print(f"  Bias Variance: {analysis['bias_variance']:.3f}")
    print(f"  Bias Detected: {'✗ YES' if analysis['bias_detected'] else '✓ NO'}")
    
    print("  Group Accuracies:")
    for group, accuracy in analysis["group_accuracies"].items():
        print(f"    {group}: {accuracy:.3f}")

# 4. Generate Medical AI Compliance Report
def generate_medical_compliance_report(pii_violations, accuracy_metrics, clinical_results, bias_results):
    """Generate comprehensive medical AI compliance report"""
    
    report = {
        "hipaa_compliance": {
            "pii_detection_rate": len(pii_violations) / len(medical_diagnoses),
            "compliance_score": 1.0 - (len(pii_violations) / len(medical_diagnoses)),
            "violations_found": len(pii_violations),
            "auto_redaction_enabled": True
        },
        "diagnostic_accuracy": {
            "overall_accuracy": accuracy_metrics.get("bleu", 0.0),
            "clinical_accuracy": clinical_results["correct_primary_diagnosis"] / 100,
            "critical_miss_rate": clinical_results["missed_critical_findings"] / 100,
            "false_urgency_rate": clinical_results["false_positive_urgency"] / 100
        },
        "bias_assessment": {
            "age_bias_detected": bias_results["age_group"]["bias_detected"],
            "gender_bias_detected": bias_results["gender"]["bias_detected"],
            "ethnicity_bias_detected": bias_results["ethnicity"]["bias_detected"],
            "insurance_bias_detected": bias_results["insurance"]["bias_detected"],
            "max_bias_variance": max(analysis["bias_variance"] for analysis in bias_results.values())
        },
        "regulatory_compliance": {
            "hipaa_compliant": len(pii_violations) == 0,
            "fda_ready": clinical_results["correct_primary_diagnosis"] >= 85,
            "bias_testing_complete": True,
            "audit_trail_enabled": True
        }
    }
    
    return report

compliance_report = generate_medical_compliance_report(
    pii_violations, aggregated_accuracy, clinical_results, bias_results
)

print("\\n=== Medical AI Compliance Report ===")
for category, metrics in compliance_report.items():
    print(f"\\n{category.replace('_', ' ').title()}:")
    for metric, value in metrics.items():
        if isinstance(value, bool):
            status = "✓ PASS" if value else "✗ FAIL"
            print(f"  {metric.replace('_', ' ').title()}: {status}")
        elif isinstance(value, float):
            if "rate" in metric or "score" in metric:
                print(f"  {metric.replace('_', ' ').title()}: {value:.1%}")
            else:
                print(f"  {metric.replace('_', ' ').title()}: {value:.3f}")
        else:
            print(f"  {metric.replace('_', ' ').title()}: {value}")

# 5. Medical AI Model Card Generation
medical_model_card = client.generate_model_card(
    "medical-diagnostic-ai-v1",
    aggregated_accuracy,
    compliance_report,
    {
        "domain": "healthcare",
        "compliance_standards": ["HIPAA", "FDA"],
        "bias_testing": True,
        "pii_protection": True,
        "clinical_validation": True
    }
)

print("\\n=== Medical AI Model Card ===")
print(medical_model_card)

print("\\n=== Medical AI Accuracy Check Complete ===")
print("Comprehensive medical AI evaluation with HIPAA compliance, bias detection, and regulatory assessment.")`}</code>
                </pre>
              </div>
            </CardContent>
          </Card>
        </TabsContent>

      </Tabs>
    </div>
    </>
  );
};

export default UseCases;